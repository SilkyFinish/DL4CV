{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0e2e14c4",
      "metadata": {
        "id": "0e2e14c4"
      },
      "source": [
        "# EECS 498-007/598-005 Assignment 5-2: Transformers\n",
        "\n",
        "Before we start, please put your name and UMID in following format\n",
        "\n",
        ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7719a8a0",
      "metadata": {
        "id": "7719a8a0"
      },
      "source": [
        "**Your Answer:**\\\n",
        "Hello WORLD, #XXXXXXXX"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "819e7379",
      "metadata": {
        "id": "819e7379"
      },
      "source": [
        "### Transformers ([Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf))\n",
        "\n",
        "To this point we have seen RNNs, which excel at sequence to sequence task but have two major drawbacks.\n",
        "First, they can suffer from vanishing gradients for long sequences.\n",
        "Second, they can take a long time to train due to sequential dependencies between hidden states which does not take advantage of the massively parallel architecture of modern GPUs.\n",
        "The first issue is largely addressed by alternate RNN architectures (LSTMs, GRUs) but not the second.\n",
        "\n",
        "Transformers solve these problems up to a certain extent by enabling to process the input parallely during training with long sequences. Though the computation is quadratic with respect to the input sequence length, it still managable with modern GPUs.\n",
        "\n",
        "In this notebook, we will implement Transformers model step-by-step by referencing the original paper, [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). We will also use a toy dataset to solve a vector-to-vector problem which is a subset of sequence-to-sequence problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8848024",
      "metadata": {
        "id": "e8848024"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "This assignment has 4 parts. In the class we learned about Encoder based Transformers but often we use an Encoder and a Decoder for sequence to sequence task. In this notebook, you will learn how to implement an Encoder-Decoder based Transformers in a step-by-step manner. We will implement a simpler version here, where the simplicity arise from the task that we are solving, which is a vector-to-vector task. This essentially means that the length of input and output sequence is **fixed** and we dont have to worry about variable length of sequences. This makes the implementation simpler.\n",
        "\n",
        "1. **Part I (Preparation)**: We will preprocess a toy dataset that consists of input arithmetic expression and an output result of the expression\n",
        "1. **Part II (Implement Transformer blocks)**: we will look how to implement building blocks of a Transformer. It will consist of following blocks\n",
        "   1. MultiHeadAttention\n",
        "   2. FeedForward\n",
        "   3. LayerNorm\n",
        "   4. Encoder Block\n",
        "   5. Decoder Block\n",
        "1. **Part III (Data Loading)**: We will use the preprocessing functions in part I and the positional encoding module to construct the Dataloader.\n",
        "1. **Part IV (Train a model)**: In the last part we will look at how to fit the implemented Transformer model to the toy dataset.\n",
        "\n",
        "You can run all things on CPU till part 3. Part 4 requires GPU and while changing the runtime for this part, you would also have to run all the previous parts as part 4 has dependency on previous parts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e1c7486",
      "metadata": {
        "id": "3e1c7486"
      },
      "source": [
        "# Part I. Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bdbbed0",
      "metadata": {
        "id": "0bdbbed0"
      },
      "source": [
        "Before getting started we need to run some boilerplate code to set up our environment. You\"ll need to rerun this setup code each time you start the notebook.\n",
        "\n",
        "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f7dede27",
      "metadata": {
        "id": "f7dede27"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c1523c0",
      "metadata": {
        "id": "6c1523c0"
      },
      "source": [
        "### Google Colab Setup\n",
        "\n",
        "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
        "\n",
        "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ff4169c8",
      "metadata": {
        "id": "ff4169c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a73635d7-b733-4d58-f817-963bbc336ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6979e52e",
      "metadata": {
        "id": "6979e52e"
      },
      "source": [
        "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
        "\n",
        "```\n",
        "[\"eecs598\", \"a5_helper.py\", \"rnn_lstm_attention_captioning.ipynb\",  \"rnn_lstm_attention_captioning.py\", \"Transformers.py\", \"Transformers.ipynb\"]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1e1beb7a",
      "metadata": {
        "id": "1e1beb7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6c2ec6-dfb7-4df6-d7e6-24e07af3fca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a5_helper.py', 'two_digit_op.json', 'eecs598', '__pycache__', 'rnn_lstm_captioning.ipynb', 'rnn_lstm_captioning.py', 'transformers.py', 'Transformers.ipynb']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
        "# Example: If you create a WI2022 folder and put all the files under A5 folder, then \"WI2022/A5\"\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
        "\n",
        "GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\",'DL4CV','A5')\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
        "\n",
        "\n",
        "# Add to sys so we can import .py files.\n",
        "\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0ea3c66",
      "metadata": {
        "id": "c0ea3c66"
      },
      "source": [
        "Once you have successfully mounted your Google Drive and located the path to this assignment, run th following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
        "\n",
        "```\n",
        "Hello from Transformers.py!\n",
        "```\n",
        "\n",
        "as well as the last edit time for the file `Transformers.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ac76ec4e",
      "metadata": {
        "id": "ac76ec4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "4bc60e37-f34b-4c40-d621-5bec3590b1e2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'hello_transformers' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-75cd83219a40>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhello_transformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhello_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'hello_transformers' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "from transformers import hello_transformers\n",
        "\n",
        "hello_transformers()\n",
        "\n",
        "\n",
        "os.environ[\"TZ\"] = \"US/Eastern\"\n",
        "time.tzset()\n",
        "hello_transformers()\n",
        "\n",
        "transformers_path = os.path.join(GOOGLE_DRIVE_PATH, \"transformers.py\")\n",
        "transformers_edit_time = time.ctime(os.path.getmtime(transformers_path))\n",
        "print(\"transformers.py last edited on %s\" % transformers_edit_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2cbf5f86",
      "metadata": {
        "id": "2cbf5f86"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch import nn\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from eecs598.utils import (\n",
        "    reset_seed,\n",
        "    tensor_to_image,\n",
        "    attention_visualizer,\n",
        ")\n",
        "from eecs598.grad import rel_error, compute_numeric_gradient\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "# for plotting\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
        "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
        "plt.rcParams[\"image.cmap\"] = \"gray\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b807888a",
      "metadata": {
        "id": "b807888a"
      },
      "source": [
        "We will use the GPU to accelerate our computation. Run this cell to make sure you are using a GPU.\n",
        "\n",
        "We will be using `torch.float = torch.float32` for data and `torch.long = torch.int64` for labels.\n",
        "\n",
        "Please refer to https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype for more details about data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0150e9a5",
      "metadata": {
        "id": "0150e9a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e147cf0-4698-49f1-d303-95c3ad96c668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good to go!\n"
          ]
        }
      ],
      "source": [
        "to_float = torch.float\n",
        "to_long = torch.long\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Good to go!\")\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
        "    DEVICE = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c2bcf33",
      "metadata": {
        "id": "5c2bcf33"
      },
      "source": [
        "### Load the toy data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16bf6435",
      "metadata": {
        "id": "16bf6435"
      },
      "source": [
        "As Transformers perform very well on sequence to sequence task, we will implement it on a toy task of Arithmetic operations. We will use transformer models to perform addition and subraction of two integers, where the absolute value of an integer is at most 50. A simple example is to perform the computation `-5 + 2` using a Transformer model and getting the corect result as `-3`. As there can be multiple ways to solve this problem, we will see how we can pose this as a sequence to sequence problem and solve it using Transformers model. Note that we had to reduce the complexity of the problem to make the Transformer work within the constrainted resources of Colab.\n",
        "\n",
        "Lets take a look at the data first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3634d982",
      "metadata": {
        "id": "3634d982"
      },
      "outputs": [],
      "source": [
        "from a5_helper import get_toy_data\n",
        "\n",
        "# load the data using helper function\n",
        "data = get_toy_data(os.path.join(GOOGLE_DRIVE_PATH,\"two_digit_op.json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f0c8b22",
      "metadata": {
        "id": "3f0c8b22"
      },
      "source": [
        "### Looking at the first four examples\n",
        "\n",
        "Below are the first four samples in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0f9a602",
      "metadata": {
        "id": "d0f9a602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e2a500-e2b4-4223-f399-8f1460ecba8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expression: BOS NEGATIVE 30 subtract NEGATIVE 34 EOS Output: BOS POSITIVE 04 EOS\n",
            "Expression: BOS NEGATIVE 34 add NEGATIVE 15 EOS Output: BOS NEGATIVE 49 EOS\n",
            "Expression: BOS NEGATIVE 28 add NEGATIVE 36 EOS Output: BOS NEGATIVE 64 EOS\n",
            "Expression: BOS POSITIVE 00 subtract POSITIVE 17 EOS Output: BOS NEGATIVE 17 EOS\n"
          ]
        }
      ],
      "source": [
        "num_examples = 4\n",
        "for q, a in zip(\n",
        "    data[\"inp_expression\"][:num_examples],\n",
        "    data[\"out_expression\"][:num_examples]\n",
        "    ):\n",
        "  print(\"Expression: \" + q + \" Output: \" + a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc93f275",
      "metadata": {
        "id": "bc93f275"
      },
      "source": [
        "## What do these examples mean:\n",
        "\n",
        "Lets look at first and third examples here and understand what they represent:\n",
        "\n",
        "- Expression: `BOS NEGATIVE 30 subtract NEGATIVE 34 EOS` Output: `BOS POSITIVE 04 EOS`: The expression here is $(-30) - (-34)$. There are two notions of the symbol `+` here: one is to denote the sign of the number and other is the operation of addition between two integers. To simplify the problem for the neural network, we have denoted them with different text tokens. The ouput of $(-30) - (-34)$ is $+4$. Here `BOS` and `EOS` refer to begining of sequence and end of sequence\n",
        "- Similarly, the second expression, `BOS NEGATIVE 34 add NEGATIVE 15 EOS` Output: `BOS NEGATIVE 49 EOS` means that we are doing the computation as $(-34) + (-15)$. As above, the symbol `-` here represents two things: first is the sign of an integer and second is the operation between two integers. Again, we have represented with different tokens to simplify the problem for the neural network. The output here is -49. Here `BOS` and `EOS` refer to begining of sequence and end of sequence\n",
        "\n",
        "Now that we have a grasp on what is the data, lets head to preprocess the data, as the neural networks don't really understand strings, we need to represent them as numbers.\n",
        "\n",
        "## Pre-processing the data\n",
        "We need to convert the raw input sequence into a format that can be processed with a neural network.\n",
        "Concretely, we need to convert a human-readable string (e.g. `BOS NEGATIVE 30 subtract NEGATIVE 34 EOS`) into a sequence of **tokens**, each of which will be an integer.\n",
        "The process of converting an input string into a sequence of tokens is known as **tokenization**.\n",
        "\n",
        "Before we can tokenize any particular sequence, we first need to build a **vocabulary**;\n",
        "this is an exhaustive list of all tokens that appear in our dataset, and a mapping from each token to a unique integer value.\n",
        "In our case, our vocabulary with consist of 16 elements: one entry for each digit `0` to `9`, two tokens to represent the sign of a number (`POSITIVE` and `NEGATIVE`), two tokens representing the addition and subtraction operations (`add`, and `subtract`), and finally two special tokens representing the start and end of the sequence (`BOS`, `EOS`).\n",
        "\n",
        "We typically represent the vocabulary with a pair of data structures.\n",
        "First is a list of all the string tokens (`vocab` below), such that `vocab[i] = s` means that the string `s` has been assigned the integer value `i`. This allows us to look up the string associated with any numeric index `i`.\n",
        "We also need a data structure that enables us to map in the other direction: given a string `s`, find the index `i` to which it has been assigned. This is typically represented as a hash map (`dict` object in Python) whose keys are strings and whose values are the indices assigned to those strings.\n",
        "You will implement the function `generate_token_dict` that inputs the list `vocab` and returns a dict `convert_str_to_token` giving this mapping.\n",
        "\n",
        "Once you have built the vocab, then you can implement the function `preprocess_input_sequence` which uses the vocab data structures to convert an input string into a list of integer tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e87a7b85",
      "metadata": {
        "id": "e87a7b85"
      },
      "outputs": [],
      "source": [
        "# Create vocab\n",
        "SPECIAL_TOKENS = [\"POSITIVE\", \"NEGATIVE\", \"add\", \"subtract\", \"BOS\", \"EOS\"]\n",
        "vocab = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"] + SPECIAL_TOKENS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "066cd35e",
      "metadata": {
        "id": "066cd35e"
      },
      "source": [
        "To generate the hash map and then process the input string using them, complete the `generate_token_dict`, `prepocess_input_sequence` functions in the python files for this exercise:\n",
        "\n",
        "You should see exact zero errors here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7beaeebe-76ac-4e59-a244-250c5a18637b",
      "metadata": {
        "id": "7beaeebe-76ac-4e59-a244-250c5a18637b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "625adcac-2c98-4047-9cca-3972a87f173a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary created successfully!\n"
          ]
        }
      ],
      "source": [
        "def generate_token_dict(vocab):\n",
        "    \"\"\"\n",
        "    The function creates a hash map from the elements in the vocabulary to\n",
        "    to a unique positive integer value.\n",
        "\n",
        "    args:\n",
        "        vocab: This is a 1D list of strings containing all the items in the vocab\n",
        "\n",
        "    Returns:\n",
        "        token_dict: a python dictionary with key as the string item in the vocab\n",
        "            and value as a unique integer value\n",
        "    \"\"\"\n",
        "    # initialize a empty dictionary\n",
        "    token_dict = {}\n",
        "    ##############################################################################\n",
        "    # TODO: Use this function to assign a unique whole number element to each    #\n",
        "    # element present in the vocab list. To do this, map the first element in the#\n",
        "    # vocab to 0 and the last element in the vocab to len(vocab), and the        #\n",
        "    # elements in between as consequetive number.                                #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    for index, token in enumerate(vocab):\n",
        "        token_dict[token] = index\n",
        "    ##############################################################################\n",
        "    #               END OF YOUR CODE                                             #\n",
        "    ##############################################################################\n",
        "    return token_dict\n",
        "\n",
        "convert_str_to_tokens = generate_token_dict(vocab)\n",
        "\n",
        "try:\n",
        "    assert convert_str_to_tokens[\"0\"] == 0\n",
        "except:\n",
        "    print(\"The first element does not map to 0. Please check the implementation\")\n",
        "\n",
        "try:\n",
        "    assert convert_str_to_tokens[\"EOS\"] == 15\n",
        "except:\n",
        "    print(\"The last element does not map to 2004. Please check the implementation\")\n",
        "\n",
        "print(\"Dictionary created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5b6e4fca",
      "metadata": {
        "id": "5b6e4fca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f84d4d45-3ddb-403e-b660-83b31f5bc5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preprocess input token error 1:  0.0\n",
            "preprocess input token error 2:  0.0\n",
            "preprocess input token error 3:  0.0\n",
            "preprocess input token error 4:  0.0\n",
            "\n",
            "\n",
            "preprocess output token error 1:  0.0\n",
            "preprocess output token error 2:  0.0\n",
            "preprocess output token error 3:  0.0\n",
            "preprocess output token error 4:  0.0\n"
          ]
        }
      ],
      "source": [
        "def generate_token_dict(vocab):\n",
        "    \"\"\"\n",
        "    The function creates a hash map from the elements in the vocabulary to\n",
        "    to a unique positive integer value.\n",
        "\n",
        "    args:\n",
        "        vocab: This is a 1D list of strings containing all the items in the vocab\n",
        "\n",
        "    Returns:\n",
        "        token_dict: a python dictionary with key as the string item in the vocab\n",
        "            and value as a unique integer value\n",
        "    \"\"\"\n",
        "    # initialize a empty dictionary\n",
        "    token_dict = {}\n",
        "    ##############################################################################\n",
        "    # TODO: Use this function to assign a unique whole number element to each    #\n",
        "    # element present in the vocab list. To do this, map the first element in the#\n",
        "    # vocab to 0 and the last element in the vocab to len(vocab), and the        #\n",
        "    # elements in between as consequetive number.                                #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    for index, token in enumerate(vocab):\n",
        "        token_dict[token] = index\n",
        "    ##############################################################################\n",
        "    #               END OF YOUR CODE                                             #\n",
        "    ##############################################################################\n",
        "    return token_dict\n",
        "def prepocess_input_sequence(\n",
        "    input_str: str, token_dict: dict, spc_tokens: list\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    The goal of this fucntion is to convert an input string into a list of positive\n",
        "    integers that will enable us to process the string using neural nets further. We\n",
        "    will use the dictionary made in the previous function to map the elements in the\n",
        "    string to a unique value. Keep in mind that we assign a value for each integer\n",
        "    present in the input sequence. For example, for a number present in the input\n",
        "    sequence \"33\", you should break it down to a list of digits,\n",
        "    ['0', '3'] and assign it to a corresponding value in the token_dict.\n",
        "\n",
        "    args:\n",
        "        input_str: A single string in the input data\n",
        "                 e.g.: \"BOS POSITIVE 0333 add POSITIVE 0696 EOS\"\n",
        "\n",
        "        token_dict: The token dictionary having key as elements in the string and\n",
        "            value as a unique positive integer. This is generated  using\n",
        "            generate_token_dict fucntion\n",
        "\n",
        "        spc_tokens: The special tokens apart from digits.\n",
        "    Returns:\n",
        "        out_tokens: a list of integers corresponding to the input string\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    ##############################################################################\n",
        "    # TODO: for each number present in the input sequence, break it down into a\n",
        "    # list of digits and use this list of digits to assign an appropriate value\n",
        "    # from token_dict. For special tokens present in the input string, assign an\n",
        "    # appropriate value for the complete token.\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    words = input_str.split()\n",
        "\n",
        "    for word in words:\n",
        "        # Check if the word is a special token\n",
        "        if word in spc_tokens:\n",
        "            out.append(token_dict[word])\n",
        "        # Check if the word is a number and break it down into digits\n",
        "        elif word.isdigit():\n",
        "            for digit in word:\n",
        "                out.append(token_dict[digit])\n",
        "        else:\n",
        "            out.append(token_dict[word])\n",
        "    ##############################################################################\n",
        "    #               END OF YOUR CODE                                             #\n",
        "    ##############################################################################\n",
        "    return out\n",
        "\n",
        "convert_str_to_tokens = generate_token_dict(vocab)\n",
        "\n",
        "ex1_in = \"BOS POSITIVE 0333 add POSITIVE 0696 EOS\"\n",
        "ex2_in = \"BOS POSITIVE 0673 add POSITIVE 0675 EOS\"\n",
        "ex3_in = \"BOS NEGATIVE 0286 subtract NEGATIVE 0044 EOS\"\n",
        "ex4_in = \"BOS NEGATIVE 0420 add POSITIVE 0342 EOS\"\n",
        "\n",
        "ex1_out = \"BOS POSITIVE 1029 EOS\"\n",
        "ex2_out = \"BOS POSITIVE 1348 EOS\"\n",
        "ex3_out = \"BOS NEGATIVE 0242 EOS\"\n",
        "ex4_out = \"BOS NEGATIVE 0078 EOS\"\n",
        "\n",
        "ex1_inp_preprocessed = torch.tensor(\n",
        "    prepocess_input_sequence(ex1_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
        ")\n",
        "ex2_inp_preprocessed = torch.tensor(\n",
        "    prepocess_input_sequence(ex2_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
        ")\n",
        "ex3_inp_preprocessed = torch.tensor(\n",
        "    prepocess_input_sequence(ex3_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
        ")\n",
        "ex4_inp_preprocessed = torch.tensor(\n",
        "    prepocess_input_sequence(ex4_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
        ")\n",
        "\n",
        "ex1_processed_expected = torch.tensor([14, 10, 0, 3, 3, 3, 12, 10, 0, 6, 9, 6, 15])\n",
        "ex2_processed_expected = torch.tensor([14, 10, 0, 6, 7, 3, 12, 10, 0, 6, 7, 5, 15])\n",
        "ex3_processed_expected = torch.tensor([14, 11, 0, 2, 8, 6, 13, 11, 0, 0, 4, 4, 15])\n",
        "ex4_processed_expected = torch.tensor([14, 11, 0, 4, 2, 0, 12, 10, 0, 3, 4, 2, 15])\n",
        "\n",
        "ex1_out = torch.tensor(\n",
        "    prepocess_input_sequence(ex1_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
        ")\n",
        "ex2_out = torch.tensor(\n",
        "    prepocess_input_sequence(ex2_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
        ")\n",
        "ex3_out = torch.tensor(\n",
        "    prepocess_input_sequence(ex3_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
        ")\n",
        "ex4_out = torch.tensor(\n",
        "    prepocess_input_sequence(ex4_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
        ")\n",
        "\n",
        "ex1_out_expected = torch.tensor([14, 10, 1, 0, 2, 9, 15])\n",
        "ex2_out_expected = torch.tensor([14, 10, 1, 3, 4, 8, 15])\n",
        "ex3_out_expected = torch.tensor([14, 11, 0, 2, 4, 2, 15])\n",
        "ex4_out_expected = torch.tensor([14, 11, 0, 0, 7, 8, 15])\n",
        "\n",
        "print(\n",
        "    \"preprocess input token error 1: \",\n",
        "    rel_error(ex1_processed_expected, ex1_inp_preprocessed),\n",
        ")\n",
        "print(\n",
        "    \"preprocess input token error 2: \",\n",
        "    rel_error(ex2_processed_expected, ex2_inp_preprocessed),\n",
        ")\n",
        "print(\n",
        "    \"preprocess input token error 3: \",\n",
        "    rel_error(ex3_processed_expected, ex3_inp_preprocessed),\n",
        ")\n",
        "print(\n",
        "    \"preprocess input token error 4: \",\n",
        "    rel_error(ex4_processed_expected, ex4_inp_preprocessed),\n",
        ")\n",
        "print(\"\\n\")\n",
        "print(\"preprocess output token error 1: \", rel_error(ex1_out_expected, ex1_out))\n",
        "print(\"preprocess output token error 2: \", rel_error(ex2_out_expected, ex2_out))\n",
        "print(\"preprocess output token error 3: \", rel_error(ex3_out_expected, ex3_out))\n",
        "print(\"preprocess output token error 4: \", rel_error(ex4_out_expected, ex4_out))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58aed0ae",
      "metadata": {
        "id": "58aed0ae"
      },
      "source": [
        "# Part II.  Implementing Transformer building blocks\n",
        "\n",
        "Now that we have looked at the data, the task is to predict the output sequence (final result), something like `NEGATIVE 42` given the input sequence (of the arthmetic expression), something like `NEGATIVE 48 subtract NEGATIVE 6`.\n",
        "\n",
        "In this section, we will look at implementing various building blocks used for implementing Transformer model. This will then be used to make Transformer encoder and decoder, which will ultimately lead us to implementing the complete Transfromer model.\n",
        "Each block will be implemented as a subclass of `nn.Module`; we will use PyTorch autograd to compute gradients, so we don't need to implement backward passes manually.\n",
        "\n",
        "We will implement the following blocks, by referencing the original paper:\n",
        "\n",
        "1. MultHeadAttention Block\n",
        "2. FeedForward Block\n",
        "3. Layer Normalization\n",
        "4. Positional Encoding block\n",
        "\n",
        "We will then use these building blocks, combined with the input embedding layer to construct the Transformer Encoder and Decoder. We will start with MultiHeadAttention block, FeedForward Block, and Layer Normalization and look at Position encoding and input embedding later.\n",
        "\n",
        "**Note:** One thing to keep in mind while implementing these blocks is that the shape of input and output Tensor from all these blocks we will be same. It always helps by checking the shapes of inputp and output tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "823cc3b9-d483-4d42-a495-eb432d1417e1",
      "metadata": {
        "id": "823cc3b9-d483-4d42-a495-eb432d1417e1"
      },
      "source": [
        "### MultiHeadAttention Block"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34036573-905d-44f0-8094-71539563ae24",
      "metadata": {
        "id": "34036573-905d-44f0-8094-71539563ae24"
      },
      "source": [
        "The image below highlights the MultiHead Attention block inside the Transformer model.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1DwU3BJsA0mUWTWlXNtNolB4oc5K4Z9PE\" alt=\"multihead_attention\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c56e134",
      "metadata": {
        "id": "5c56e134"
      },
      "source": [
        "Transformers are sequence to sequence networks i.e., we get a sequence (for example a sentence in English) and get output a sequence (for example a sentence in Spanish). The input sequence are first transformed into embeddings as discussed in the RNN section and these embeddings are then passed through a Positional Encoding block. The resultant Embeddings are then transformed into three vectors, *query*, *key*, and *value* using learnable weights and we then use a Transformer Encoder and Decoder to get the final output sequence. For this section, we will assume that we have the *query*, *key*, and the *value* vector and work on them.\n",
        "\n",
        "In the above figure, you can see that the Encoder has multihead attention block is right after these blocks. There is also a masked multihead attention in the deocoder but we will see that it's easy to implement the masked attention when we have implemented the basic MultiHeadAttention block.\n",
        "To implement the basic MultiheadAttention block, we will first implement the Self Attention block and see that MultiHeadAttention can be implemented as a direct extension of the Self Attention block.\n",
        "\n",
        "## Self Attention Block\n",
        "\n",
        "Taking inspiration from information retreival paradigm, Transformers have this notion of *query*, *key*, and *value* where given a *query* we try extract information from *key*-*value* pairs. Moving along those lines, we perform this mathematically by taking the weighted sum of *values* for each *query*, where weight is computed by dot product of *query* and the *key*. More precisely, for each query we compute the dot product with all the keys and then use the scalar output of those dot products as weights to find the weighted sum of *values*. Note that before finding the weighted sum, we also apply softmax function to the weights vector. Lets start with implementing of Attention Block that takes input as *query*, *key*, and *value* vectors and returns a Tensor, that is weighted sum of the *values*.\n",
        "\n",
        "For this section, you need to implement three functions, `scaled_dot_product_two_loop_single`, `scaled_dot_product_two_loop_batch`, and `scaled_dot_product_no_loop_batch` inside the transformers.py file. This might look very similar to the `dot_product_attention` in the RNN notebook but there is a subtle difference in the inputs. You should see the errors of the order less than 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "747894ed",
      "metadata": {
        "id": "747894ed"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_two_loop_single(\n",
        "    query: Tensor, key: Tensor, value: Tensor\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    The function performs a fundamental block for attention mechanism, the scaled\n",
        "    dot product. We map the input query, key, and value to the output. Follow the\n",
        "    description in TODO for implementation.\n",
        "\n",
        "    args:\n",
        "        query: a Tensor of shape (K, M) where K is the sequence length and M is\n",
        "            the sequence embeding dimension\n",
        "\n",
        "        key: a Tensor of shape (K, M) where K is the sequence length and M is the\n",
        "            sequence embeding dimension\n",
        "\n",
        "        value: a Tensor of shape (K, M) where K is the sequence length and M is\n",
        "            the sequence embeding dimension\n",
        "\n",
        "\n",
        "    Returns\n",
        "        out: a tensor of shape (K, M) which is the output of self-attention from\n",
        "        the function\n",
        "    \"\"\"\n",
        "    # make a placeholder for the output\n",
        "    out = None\n",
        "    ###############################################################################\n",
        "    # TODO: Implement this function using exactly two for loops. For each of the  #\n",
        "    # K queries, compute its dot product with each of the K keys. The scalar      #\n",
        "    # output of the dot product will the be scaled by dividing it with the sqrt(M)#\n",
        "    # Once we get all the K scaled weights corresponding to a query, we apply a   #\n",
        "    # softmax function on them and use the value matrix to compute the weighted   #\n",
        "    # sum of values using the matrix-vector product. This single vector computed  #\n",
        "    # using weighted sum becomes an output to the Kth query vector                #\n",
        "    ###############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    # Sequence length (K) and embedding dimension (M)\n",
        "    K, M = query.shape\n",
        "    # Scaling factor\n",
        "    scale = M**0.5\n",
        "    # Placeholder for output\n",
        "    out = torch.zeros_like(query)\n",
        "\n",
        "    # Iterate over each query vector\n",
        "    for i in range(K):\n",
        "        # Compute scaled dot product between query[i] and each key\n",
        "        attention_scores = torch.zeros(K)\n",
        "        for j in range(K):\n",
        "            attention_scores[j] = torch.dot(query[i], key[j]) / scale\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = torch.softmax(attention_scores, dim=0)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out[i] = torch.sum(attention_weights.unsqueeze(1) * value, dim=0)\n",
        "    ##############################################################################\n",
        "    #               END OF YOUR CODE                                             #\n",
        "    ##############################################################################\n",
        "    return out\n",
        "\n",
        "\n",
        "def scaled_dot_product_two_loop_batch(\n",
        "    query: Tensor, key: Tensor, value: Tensor\n",
        ") -> Tensor:\n",
        "\n",
        "    \"\"\"\n",
        "    The function performs a fundamental block for attention mechanism, the scaled\n",
        "    dot product. We map the input query, key, and value to the output. Follow the\n",
        "    description in TODO for implementation.\n",
        "\n",
        "    args:\n",
        "        query: a Tensor of shape (N,K, M) where N is the batch size, K is the\n",
        "            sequence length and  M is the sequence embeding dimension\n",
        "\n",
        "        key: a Tensor of shape (N, K, M) where N is the batch size, K is the\n",
        "            sequence length and M is the sequence embeding dimension\n",
        "\n",
        "\n",
        "        value: a Tensor of shape (N, K, M) where N is the batch size, K is the\n",
        "            sequence length and M is the sequence embeding dimension\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        out: a tensor of shape (N, K, M) that contains the weighted sum of values\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    # make a placeholder for the output\n",
        "    out = None\n",
        "    N, K, M = query.shape\n",
        "    ###############################################################################\n",
        "    # TODO: This function is extedning self_attention_two_loop_single for a batch #\n",
        "    # of N. Implement this function using exactly two for loops. For each N       #\n",
        "    # we have a query, key and value. The final output is the weighted sum of     #\n",
        "    # values of these N queries and keys. The weight here is computed using scaled#\n",
        "    # dot product  between each of the K queries and key. The scaling value here  #\n",
        "    # is sqrt(M). For each of the N sequences, compute the softmaxed weights and  #\n",
        "    # use them to compute weighted average of value matrix.                       #\n",
        "    # Hint: look at torch.bmm                                                     #\n",
        "    ###############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    # Scaling factor\n",
        "    scale = M**0.5\n",
        "    # Placeholder for output\n",
        "    out = torch.zeros_like(query)\n",
        "\n",
        "    # Iterate over each query vector\n",
        "    for i in range(K):\n",
        "        # Compute scaled dot product between query[i] and each key\n",
        "        attention_scores = torch.zeros(N,K)\n",
        "        for j in range(K):\n",
        "            attention_scores[:,j] = (query[:,i]*key[:,j]).sum(dim=1) / scale\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        out[:, i] = torch.bmm(attention_weights.unsqueeze(1), value).squeeze(1)  # (N, M)\n",
        "    ##############################################################################\n",
        "    #               END OF YOUR CODE                                             #\n",
        "    ##############################################################################\n",
        "    return out\n",
        "\n",
        "\n",
        "def scaled_dot_product_no_loop_batch(\n",
        "    query: Tensor, key: Tensor, value: Tensor, mask: Tensor = None\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "\n",
        "    The function performs a fundamental block for attention mechanism, the scaled\n",
        "    dot product. We map the input query, key, and value to the output. It uses\n",
        "    Matrix-matrix multiplication to find the scaled weights and then matrix-matrix\n",
        "    multiplication to find the final output.\n",
        "\n",
        "    args:\n",
        "        query: a Tensor of shape (N,K, M) where N is the batch size, K is the\n",
        "            sequence length and M is the sequence embeding dimension\n",
        "\n",
        "        key:  a Tensor of shape (N, K, M) where N is the batch size, K is the\n",
        "            sequence length and M is the sequence embeding dimension\n",
        "\n",
        "\n",
        "        value: a Tensor of shape (N, K, M) where N is the batch size, K is the\n",
        "            sequence length and M is the sequence embeding dimension\n",
        "\n",
        "\n",
        "        mask: a Bool Tensor of shape (N, K, K) that is used to mask the weights\n",
        "            used for computing weighted sum of values\n",
        "\n",
        "\n",
        "    return:\n",
        "        y: a tensor of shape (N, K, M) that contains the weighted sum of values\n",
        "\n",
        "        weights_softmax: a tensor of shape (N, K, K) that contains the softmaxed\n",
        "            weight matrix.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _, _, M = query.shape\n",
        "    y = None\n",
        "    weights_softmax = None\n",
        "    ###############################################################################\n",
        "    # TODO: This function performs same function as self_attention_two_loop_batch #\n",
        "    # Implement this function using no loops.                                     #\n",
        "    # For the mask part, you can ignore it for now and revisit it in the later part.\n",
        "    # Given the shape of the mask is (N, K, K), and it is boolean with True values#\n",
        "    # indicating  the weights that have to be masked and False values indicating  #\n",
        "    # the weghts that dont need to be masked at that position. These masked-scaled#\n",
        "    # weights can then be softmaxed to compute the final weighted sum of values   #\n",
        "    # Hint: look at torch.bmm and torch.masked_fill                               #\n",
        "    ###############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    scale = M**0.5\n",
        "    attention_scores = torch.bmm(query, key.transpose(1, 2)) / scale\n",
        "    if mask is not None:\n",
        "        ##########################################################################\n",
        "        # TODO: Apply the mask to the weight matrix by assigning -1e9 to the     #\n",
        "        # positions where the mask value is True, otherwise keep it as it is.    #\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        attention_scores = attention_scores.masked_fill(mask, -1e9)\n",
        "    # Replace \"pass\" statement with your code\n",
        "    # Apply softmax to get normalized attention weights\n",
        "    weights_softmax = torch.softmax(attention_scores, dim=-1)  # (N, K, K)\n",
        "\n",
        "    # Compute weighted sum of values\n",
        "    # (N, K, K) @ (N, K, M) -> (N, K, M)\n",
        "    y = torch.bmm(weights_softmax, value)\n",
        "    ##############################################################################\n",
        "    #               END OF YOUR CODE                                             #\n",
        "    ##############################################################################\n",
        "    return y, weights_softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7341f6ff",
      "metadata": {
        "id": "7341f6ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18005c97-873d-4e19-b895-b1974957bb12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sacled_dot_product_two_loop_single error:  5.204997336388729e-06\n"
          ]
        }
      ],
      "source": [
        "N = 2  # Number of sentences\n",
        "K = 5  # Number of words in a sentence\n",
        "M = 4  # feature dimension of each word embedding\n",
        "\n",
        "query = torch.linspace(-0.4, 0.6, steps=K * M).reshape(K, M)  # **to_double_cuda\n",
        "key = torch.linspace(-0.8, 0.5, steps=K * M).reshape(K, M)  # **to_double_cuda\n",
        "value = torch.linspace(-0.3, 0.8, steps=K * M).reshape(K, M)  # *to_double_cuda\n",
        "\n",
        "y = scaled_dot_product_two_loop_single(query, key, value)\n",
        "y_expected = torch.tensor(\n",
        "    [\n",
        "        [0.08283, 0.14073, 0.19862, 0.25652],\n",
        "        [0.13518, 0.19308, 0.25097, 0.30887],\n",
        "        [0.18848, 0.24637, 0.30427, 0.36216],\n",
        "        [0.24091, 0.29881, 0.35670, 0.41460],\n",
        "        [0.29081, 0.34871, 0.40660, 0.46450],\n",
        "    ]\n",
        ").to(torch.float32)\n",
        "print(\"sacled_dot_product_two_loop_single error: \", rel_error(y_expected, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b11340",
      "metadata": {
        "id": "79b11340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b1aaeb-db08-45c9-c660-7a8a2bcc4256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scaled_dot_product_two_loop_batch error:  4.020571992067902e-06\n"
          ]
        }
      ],
      "source": [
        "N = 2  # Number of sentences\n",
        "K = 5  # Number of words in a sentence\n",
        "M = 4  # feature dimension of each word embedding\n",
        "\n",
        "query = torch.linspace(-0.4, 0.6, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
        "key = torch.linspace(-0.8, 0.5, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
        "value = torch.linspace(-0.3, 0.8, steps=N * K * M).reshape(N, K, M)  # *to_double_cuda\n",
        "\n",
        "y = scaled_dot_product_two_loop_batch(query, key, value)\n",
        "y_expected = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [-0.09603, -0.06782, -0.03962, -0.01141],\n",
        "            [-0.08991, -0.06170, -0.03350, -0.00529],\n",
        "            [-0.08376, -0.05556, -0.02735, 0.00085],\n",
        "            [-0.07760, -0.04939, -0.02119, 0.00702],\n",
        "            [-0.07143, -0.04322, -0.01502, 0.01319],\n",
        "        ],\n",
        "        [\n",
        "            [0.49884, 0.52705, 0.55525, 0.58346],\n",
        "            [0.50499, 0.53319, 0.56140, 0.58960],\n",
        "            [0.51111, 0.53931, 0.56752, 0.59572],\n",
        "            [0.51718, 0.54539, 0.57359, 0.60180],\n",
        "            [0.52321, 0.55141, 0.57962, 0.60782],\n",
        "        ],\n",
        "    ]\n",
        ").to(torch.float32)\n",
        "print(\"scaled_dot_product_two_loop_batch error: \", rel_error(y_expected, y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b13b2d96",
      "metadata": {
        "id": "b13b2d96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42465280-26b6-441d-d483-63b0608e30da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scaled_dot_product_no_loop_batch error:  4.020571992067902e-06\n"
          ]
        }
      ],
      "source": [
        "N = 2  # Number of sentences\n",
        "K = 5  # Number of words in a sentence\n",
        "M = 4  # feature dimension of each word embedding\n",
        "\n",
        "query = torch.linspace(-0.4, 0.6, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
        "key = torch.linspace(-0.8, 0.5, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
        "value = torch.linspace(-0.3, 0.8, steps=N * K * M).reshape(N, K, M)  # *to_double_cuda\n",
        "\n",
        "\n",
        "y, _ = scaled_dot_product_no_loop_batch(query, key, value)\n",
        "\n",
        "y_expected = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [-0.09603, -0.06782, -0.03962, -0.01141],\n",
        "            [-0.08991, -0.06170, -0.03350, -0.00529],\n",
        "            [-0.08376, -0.05556, -0.02735, 0.00085],\n",
        "            [-0.07760, -0.04939, -0.02119, 0.00702],\n",
        "            [-0.07143, -0.04322, -0.01502, 0.01319],\n",
        "        ],\n",
        "        [\n",
        "            [0.49884, 0.52705, 0.55525, 0.58346],\n",
        "            [0.50499, 0.53319, 0.56140, 0.58960],\n",
        "            [0.51111, 0.53931, 0.56752, 0.59572],\n",
        "            [0.51718, 0.54539, 0.57359, 0.60180],\n",
        "            [0.52321, 0.55141, 0.57962, 0.60782],\n",
        "        ],\n",
        "    ]\n",
        ").to(torch.float32)\n",
        "\n",
        "print(\"scaled_dot_product_no_loop_batch error: \", rel_error(y_expected, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56e21e2d",
      "metadata": {
        "id": "56e21e2d"
      },
      "source": [
        "## Observing time complexity:\n",
        "\n",
        "As Transformers are infamous for their time complexity that depends on the size of the input sequence.\n",
        "We can verify this now that we have implemented `self_attention_no_loop`.\n",
        "Run the cells below: the first has a sequence length of 256 and the second one has a sequence length of 512. You should roughly be 4 times slower with sequence length 512, hence showing that compleixity of the transformers increase quadratically with resprect to increase in the in sequence length.\n",
        "The `%timeit` lines may take several seconds to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7d58596",
      "metadata": {
        "id": "e7d58596",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a6b7bd-adeb-4b35-dbce-9c989b0fd9b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "416 ms ± 1.1 ms per loop (mean ± std. dev. of 2 runs, 5 loops each)\n"
          ]
        }
      ],
      "source": [
        "N = 64\n",
        "K = 256  # defines the input sequence length\n",
        "M = emb_size = 2048\n",
        "dim_q = dim_k = 2048\n",
        "query = torch.linspace(-0.4, 0.6, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
        "key = torch.linspace(-0.8, 0.5, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
        "value = torch.linspace(-0.3, 0.8, steps=N * K * M).reshape(N, K, M)  # *to_double_cuda\n",
        "\n",
        "%timeit -n 5 -r 2  y = scaled_dot_product_no_loop_batch(query, key, value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a85adf69",
      "metadata": {
        "id": "a85adf69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b4b81ea-ca53-45e3-956d-644d96024508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.56 s ± 59.9 ms per loop (mean ± std. dev. of 2 runs, 5 loops each)\n"
          ]
        }
      ],
      "source": [
        "N = 64\n",
        "K = 512  # defines the input requence length\n",
        "M = emb_size = 2048\n",
        "dim_q = dim_k = 2048\n",
        "query = torch.linspace(-0.4, 0.6, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
        "key = torch.linspace(-0.8, 0.5, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
        "value = torch.linspace(-0.3, 0.8, steps=N * K * M).reshape(N, K, M)  # *to_double_cuda\n",
        "\n",
        "%timeit -n 5 -r 2  y = scaled_dot_product_no_loop_batch(query, key, value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d6e92b0",
      "metadata": {
        "id": "9d6e92b0"
      },
      "source": [
        "Now that we have implemented `scaled_dot_product_no_loop_batch`, lets implement `SingleHeadAttention`, that will serve as a building block for the `MultiHeadAttention` block. For this exercise, we have made a `SingleHeadAttention` class that inherits from `nn.module` class of Pytorch. You need to implement the `__init__` and the `forward` functions inside `Transformers.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5af8c01-dc3a-4cca-a479-e07580914e4c",
      "metadata": {
        "id": "d5af8c01-dc3a-4cca-a479-e07580914e4c"
      },
      "source": [
        "Run the following cells to test your implementation of `SelfAttention` layer. We have also written code to check the backward pass using pytorch autograd API in the following cell. You should expect the error to be less than 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "113b3bf5",
      "metadata": {
        "id": "113b3bf5"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, dim_in: int, dim_q: int, dim_v: int):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        This class encapsulates the implementation of self-attention layer. We map\n",
        "        the input query, key, and value using MLP layers and then use\n",
        "        scaled_dot_product_no_loop_batch to the final output.\n",
        "\n",
        "        args:\n",
        "            dim_in: an int value for input sequence embedding dimension\n",
        "            dim_q: an int value for output dimension of query and ley vector\n",
        "            dim_v: an int value for output dimension for value vectors\n",
        "\n",
        "        \"\"\"\n",
        "        self.q = None  # initialize for query\n",
        "        self.k = None  # initialize for key\n",
        "        self.v = None  # initialize for value\n",
        "        self.weights_softmax = None\n",
        "        ##########################################################################\n",
        "        # TODO: This function initializes three functions to transform the 3 input\n",
        "        # sequences to key, query and value vectors. More precisely, initialize  #\n",
        "        # three nn.Linear layers that can transform the input with dimension     #\n",
        "        # dim_in to query with dimension dim_q, key with dimension dim_q, and    #\n",
        "        # values with dim_v. For each Linear layer, use the following strategy to#\n",
        "        # initialize the weights:                                                #\n",
        "        # If a Linear layer has input dimension D_in and output dimension D_out  #\n",
        "        # then initialize the weights sampled from a uniform distribution bounded#\n",
        "        # by [-c, c]                                                             #\n",
        "        # where c = sqrt(6/(D_in + D_out))                                       #\n",
        "        # Please use the same names for query, key and value transformations     #\n",
        "        # as given above. self.q, self.k, and self.v respectively.               #\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        # Initialize Linear layers for query, key, and value transformations\n",
        "        self.q = nn.Linear(dim_in, dim_q)\n",
        "        self.k = nn.Linear(dim_in, dim_q)\n",
        "        self.v = nn.Linear(dim_in, dim_v)\n",
        "\n",
        "        # Custom weight initialization for each Linear layer\n",
        "        for layer in [self.q, self.k, self.v]:\n",
        "            D_in, D_out = layer.in_features, layer.out_features\n",
        "            bound = (6 / (D_in + D_out))**0.5  # Calculate the bound\n",
        "            nn.init.uniform_(layer.weight, -bound, bound)  # Initialize weights\n",
        "            if layer.bias is not None:\n",
        "                nn.init.uniform_(layer.bias, -bound, bound)  # Initialize bias if it exists\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "\n",
        "    def forward(\n",
        "        self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor = None\n",
        "    ) -> Tensor:\n",
        "\n",
        "        \"\"\"\n",
        "        An implementation of the forward pass of the self-attention layer.\n",
        "\n",
        "        args:\n",
        "            query: Tensor of shape (N, K, M)\n",
        "            key: Tensor of shape (N, K, M)\n",
        "            value: Tensor of shape (N, K, M)\n",
        "            mask: Tensor of shape (N, K, K)\n",
        "        return:\n",
        "            y: Tensor of shape (N, K, dim_v)\n",
        "        \"\"\"\n",
        "        self.weights_softmax = (\n",
        "            None  # weight matrix after applying self_attention_no_loop_batch\n",
        "        )\n",
        "        y = None\n",
        "        ##########################################################################\n",
        "        # TODO: Use the functions initialized in the init fucntion to find the   #\n",
        "        # output tensors. Precisely, pass the inputs query, key and value to the #\n",
        "        #  three functions iniitalized above. Then, pass these three transformed #\n",
        "        # query,  key and value tensors to the self_attention_no_loop_batch to   #\n",
        "        # get the final output. For now, dont worry about the mask and just      #\n",
        "        # pass it as a variable in self_attention_no_loop_batch. Assign the value#\n",
        "        # of output weight matrix from self_attention_no_loop_batch to the       #\n",
        "        # variable self.weights_softmax                                          #\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        query=self.q(query)\n",
        "        key=self.k(key)\n",
        "        value=self.v(value)\n",
        "        y,self.weights_softmax=scaled_dot_product_no_loop_batch(query,key,value)\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc3669f2",
      "metadata": {
        "id": "dc3669f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67b8923-f33b-4551-ef69-a49a85696ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfAttention error:  5.567700453666357e-07\n",
            "SelfAttention error:  0.4621903063309185\n"
          ]
        }
      ],
      "source": [
        "reset_seed(0)\n",
        "N = 2\n",
        "K = 4\n",
        "M = emb_size = 4\n",
        "dim_q = dim_k = 4\n",
        "atten_single = SelfAttention(emb_size, dim_q, dim_k)\n",
        "\n",
        "for k, v in atten_single.named_parameters():\n",
        "    # print(k, v.shape) # uncomment this to see the weight shape\n",
        "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
        "\n",
        "query = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
        "    N, K, M\n",
        ")  # **to_double_cuda\n",
        "key = torch.linspace(-0.8, 0.5, steps=N * K * M, requires_grad=True).reshape(\n",
        "    N, K, M\n",
        ")  # **to_double_cuda\n",
        "value = torch.linspace(-0.3, 0.8, steps=N * K * M, requires_grad=True).reshape(\n",
        "    N, K, M\n",
        ")  # *to_double_cuda\n",
        "\n",
        "query.retain_grad()\n",
        "key.retain_grad()\n",
        "value.retain_grad()\n",
        "\n",
        "y_expected = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [-1.10382, -0.37219, 0.35944, 1.09108],\n",
        "            [-1.45792, -0.50067, 0.45658, 1.41384],\n",
        "            [-1.74349, -0.60428, 0.53493, 1.67414],\n",
        "            [-1.92584, -0.67044, 0.58495, 1.84035],\n",
        "        ],\n",
        "        [\n",
        "            [-4.59671, -1.63952, 1.31767, 4.27486],\n",
        "            [-4.65586, -1.66098, 1.33390, 4.32877],\n",
        "            [-4.69005, -1.67339, 1.34328, 4.35994],\n",
        "            [-4.71039, -1.68077, 1.34886, 4.37848],\n",
        "        ],\n",
        "    ]\n",
        ")\n",
        "\n",
        "dy_expected = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [-0.09084, -0.08961, -0.08838, -0.08715],\n",
        "            [0.69305, 0.68366, 0.67426, 0.66487],\n",
        "            [-0.88989, -0.87783, -0.86576, -0.85370],\n",
        "            [0.25859, 0.25509, 0.25158, 0.24808],\n",
        "        ],\n",
        "        [\n",
        "            [-0.05360, -0.05287, -0.05214, -0.05142],\n",
        "            [0.11627, 0.11470, 0.11312, 0.11154],\n",
        "            [-0.01048, -0.01034, -0.01019, -0.01005],\n",
        "            [-0.03908, -0.03855, -0.03802, -0.03749],\n",
        "        ],\n",
        "    ]\n",
        ")\n",
        "\n",
        "y = atten_single(query, key, value)\n",
        "dy = torch.randn(*y.shape)  # , **to_double_cuda\n",
        "\n",
        "y.backward(dy)\n",
        "query_grad = query.grad\n",
        "\n",
        "print(\"SelfAttention error: \", rel_error(y_expected, y))\n",
        "print(\"SelfAttention error: \", rel_error(dy_expected, query_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b80b79",
      "metadata": {
        "id": "f4b80b79"
      },
      "source": [
        "We have implemented the `SingleHeadAttention` block which brings use very close to implementing `MultiHeadAttention`. We will now see that this can be achieved by manipulating the shapes of input tensors based on number of heads in the Multi-Attention block. We design a network that uses multiple SingleHeadAttention blocks on the same input to compute the output tensors and finally concatenate them to generate a single output. This is not the implementation used in practice as it forces you to initialize multiple layers but we use it here for simplicity. Implement MultiHeadAttention block in the `transformers.py` file by using the SingleHeadAttention block."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d530dc6-cf5d-41b9-b62e-71a1095a0f01",
      "metadata": {
        "id": "3d530dc6-cf5d-41b9-b62e-71a1095a0f01"
      },
      "source": [
        "Run the following cells to test your `MultiHeadAttention` layer. Again, as `SelfAttention`, we have used pytorch autograd API to test the backward pass. You should expect error values below 1e-5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6da2558e",
      "metadata": {
        "id": "6da2558e"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads: int, dim_in: int, dim_out: int):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        A naive implementation of the MultiheadAttention layer for Transformer model.\n",
        "        We use multiple SelfAttention layers parallely on the same input and then concat\n",
        "        them to into a single tensor. This Tensor is then passed through an MLP to\n",
        "        generate the final output. The input shape will look like (N, K, M) where\n",
        "        N is the batch size, K is the batch size and M is the sequence embedding\n",
        "        dimension.\n",
        "        args:\n",
        "            num_heads: int value specifying the number of heads\n",
        "            dim_in: int value specifying the input dimension of the query, key\n",
        "                and value. This will be the input dimension to each of the\n",
        "                SingleHeadAttention blocks\n",
        "            dim_out: int value specifying the output dimension of the complete\n",
        "                MultiHeadAttention block\n",
        "\n",
        "\n",
        "\n",
        "        NOTE: Here, when we say dimension, we mean the dimesnion of the embeddings.\n",
        "              In Transformers the input is a tensor of shape (N, K, M), here N is\n",
        "              the batch size , K is the sequence length and M is the size of the\n",
        "              input embeddings. As the sequence length(K) and number of batches(N)\n",
        "              don't change usually, we mostly transform\n",
        "              the dimension(M) dimension.\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        ##########################################################################\n",
        "        # TODO: Initialize two things here:                                      #\n",
        "        # 1.) Use nn.ModuleList to initialze a list of SingleHeadAttention layer #\n",
        "        # modules.The length of this list should be equal to num_heads with each #\n",
        "        # SingleHeadAttention layer having input dimension as dim_in, and query  #\n",
        "        # , key, and value dimension as dim_out.                                 #\n",
        "        # 2.) Use nn.Linear to map the output of nn.Modulelist block back to     #\n",
        "        # dim_in. Initialize the weights using the strategy mentioned in         #\n",
        "        # SelfAttention.                                                         #\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        dim_qv = dim_out // num_heads\n",
        "        self.heads=nn.ModuleList([SelfAttention(dim_in,dim_qv,dim_qv) for i in range(num_heads)])\n",
        "        self.output_linear = nn.Linear(dim_out, dim_in)\n",
        "        # Initialize the output Linear layer weights similarly to SelfAttention\n",
        "        D_in, D_out = self.output_linear.in_features, self.output_linear.out_features\n",
        "        bound = (6 / (D_in + D_out))**0.5\n",
        "        nn.init.uniform_(self.output_linear.weight, -bound, bound)\n",
        "        if self.output_linear.bias is not None:\n",
        "            nn.init.uniform_(self.output_linear.bias, -bound, bound)\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "\n",
        "    def forward(\n",
        "        self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor = None\n",
        "    ) -> Tensor:\n",
        "\n",
        "        \"\"\"\n",
        "        An implementation of the forward pass of the MultiHeadAttention layer.\n",
        "\n",
        "        args:\n",
        "            query: Tensor of shape (N, K, M) where N is the number of sequences in\n",
        "                the batch, K is the sequence length and M is the input embedding\n",
        "                dimension. M should be equal to dim_in in the init function\n",
        "\n",
        "            key: Tensor of shape (N, K, M) where N is the number of sequences in\n",
        "                the batch, K is the sequence length and M is the input embedding\n",
        "                dimension. M should be equal to dim_in in the init function\n",
        "\n",
        "            value: Tensor of shape (N, K, M) where N is the number of sequences in\n",
        "                the batch, K is the sequence length and M is the input embedding\n",
        "                dimension. M should be equal to dim_in in the init function\n",
        "\n",
        "            mask: Tensor of shape (N, K, K) where N is the number of sequences in\n",
        "                the batch, K is the sequence length and M is the input embedding\n",
        "                dimension. M should be equal to dim_in in the init function\n",
        "\n",
        "        returns:\n",
        "            y: Tensor of shape (N, K, M)\n",
        "        \"\"\"\n",
        "        y = None\n",
        "        ##########################################################################\n",
        "        # TODO: You need to perform a forward pass through the MultiHeadAttention#\n",
        "        # block using the variables defined in the initializing function. The    #\n",
        "        # nn.ModuleList behaves as a list and you could use a for loop or list   #\n",
        "        # comprehension to extract different elements of it. Each of the elements#\n",
        "        # inside nn.ModuleList is a SingleHeadAttention that  will take the same #\n",
        "        # query, key, value and mask tensors and you will get a list of tensors as\n",
        "        # output. Concatenate this list if tensors and pass them through the     #\n",
        "        # nn.Linear mapping function defined in the initialization step.         #\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        # Apply each head to the input, and collect the results\n",
        "        head_outputs = [head(query,key,value)[0] for head in self.heads]  # Collect each head's output\n",
        "        concatenated = torch.cat(head_outputs, dim=-1)  # Concatenate along the embedding dimension\n",
        "\n",
        "        # Project concatenated outputs back to `dim_out`\n",
        "        y = self.output_linear(concatenated)\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87bf04ff",
      "metadata": {
        "id": "87bf04ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41c2fc01-7172-4310-8b5e-da92d4b7a9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiHeadAttention error:  1.0\n",
            "MultiHeadAttention error:  1.0\n"
          ]
        }
      ],
      "source": [
        "reset_seed(0)\n",
        "N = 2\n",
        "num_heads = 2\n",
        "K = 4\n",
        "M = inp_emb_size = 4\n",
        "out_emb_size = 8\n",
        "atten_multihead = MultiHeadAttention(num_heads, inp_emb_size, out_emb_size)\n",
        "\n",
        "for k, v in atten_multihead.named_parameters():\n",
        "    # print(k, v.shape) # uncomment this to see the weight shape\n",
        "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
        "\n",
        "query = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
        "    N, K, M\n",
        ")  # **to_double_cuda\n",
        "key = torch.linspace(-0.8, 0.5, steps=N * K * M, requires_grad=True).reshape(\n",
        "    N, K, M\n",
        ")  # **to_double_cuda\n",
        "value = torch.linspace(-0.3, 0.8, steps=N * K * M, requires_grad=True).reshape(\n",
        "    N, K, M\n",
        ")  # *to_double_cuda\n",
        "\n",
        "query.retain_grad()\n",
        "key.retain_grad()\n",
        "value.retain_grad()\n",
        "\n",
        "y_expected = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [-0.23104, 0.50132, 1.23367, 1.96603],\n",
        "            [0.68324, 1.17869, 1.67413, 2.16958],\n",
        "            [1.40236, 1.71147, 2.02058, 2.32969],\n",
        "            [1.77330, 1.98629, 2.19928, 2.41227],\n",
        "        ],\n",
        "        [\n",
        "            [6.74946, 5.67302, 4.59659, 3.52015],\n",
        "            [6.82813, 5.73131, 4.63449, 3.53767],\n",
        "            [6.86686, 5.76001, 4.65315, 3.54630],\n",
        "            [6.88665, 5.77466, 4.66268, 3.55070],\n",
        "        ],\n",
        "    ]\n",
        ")\n",
        "dy_expected = torch.tensor(\n",
        "    [[[ 0.56268,  0.55889,  0.55510,  0.55131],\n",
        "         [ 0.43286,  0.42994,  0.42702,  0.42411],\n",
        "         [ 2.29865,  2.28316,  2.26767,  2.25218],\n",
        "         [ 0.49172,  0.48841,  0.48509,  0.48178]],\n",
        "\n",
        "        [[ 0.25083,  0.24914,  0.24745,  0.24576],\n",
        "         [ 0.14949,  0.14849,  0.14748,  0.14647],\n",
        "         [-0.03105, -0.03084, -0.03063, -0.03043],\n",
        "         [-0.02082, -0.02068, -0.02054, -0.02040]]]\n",
        ")\n",
        "\n",
        "y = atten_multihead(query, key, value)\n",
        "dy = torch.randn(*y.shape)  # , **to_double_cuda\n",
        "\n",
        "y.backward(dy)\n",
        "query_grad = query.grad\n",
        "print(\"MultiHeadAttention error: \", rel_error(y_expected, y))\n",
        "print(\"MultiHeadAttention error: \", rel_error(dy_expected, query_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be6374e-a968-4b56-a1e4-31d745330b62",
      "metadata": {
        "id": "9be6374e-a968-4b56-a1e4-31d745330b62"
      },
      "source": [
        "### LayerNormalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a71878d-f75b-4412-ad89-5cde3100f669",
      "metadata": {
        "id": "1a71878d-f75b-4412-ad89-5cde3100f669"
      },
      "source": [
        "In the follwing image we have highlighted the portion where LayerNorm has been used in the Transformer model. Note that in the architecture diagram it's written Add & Norm but we will implement The Norm layer for now and implement the Add part in a different manner.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1baDniYuRzsEGnDegAFiARMhoxJjKSF2r\" alt=\"Layer_norm\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "524ff12e",
      "metadata": {
        "id": "524ff12e"
      },
      "source": [
        "We implemented BatchNorm while working with CNNs. One of the problems of BatchNorm is its dependency on the the complete batch which might not give good results when the batch size is small. Ba et al proposed `LayerNormalization` that takes into account these problems and has become a standard in sequence-to-sequence tasks. In this section, we will implement `LayerNormalization`. Another nice quality of `LayerNormalization` is that as it depends on individual time steps or each element of the sequence, it can be parallelized and the test time runs in a similar manner hence making it better implementation wise. Again, you have to only implement the forward pass and the backward pass will be taken care by Pytorch autograd. Implement the `LayerNormalization` class in `transformers.py`, you should expect the error below 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "cdbc1bf9",
      "metadata": {
        "id": "cdbc1bf9"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, emb_dim: int, epsilon: float = 1e-10):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        The class implements the Layer Normalization for Linear layers in\n",
        "        Transformers.  Unlike BathcNorm ,it estimates the normalization statistics\n",
        "        for each element present in the batch and hence does not depend on the\n",
        "        complete batch.\n",
        "        The input shape will look something like (N, K, M) where N is the batch\n",
        "        size, K is the sequence length and M is the sequence length embedding. We\n",
        "        compute the  mean with shape (N, K) and standard deviation with shape (N, K)\n",
        "        and use them to normalize each sequence.\n",
        "\n",
        "        args:\n",
        "            emb_dim: int representing embedding dimension\n",
        "            epsilon: float value\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        ##########################################################################\n",
        "        # TODO: Initialize the scale and shift parameters for LayerNorm.         #\n",
        "        # Initialize the scale parameters to all ones and shift parameter to all #\n",
        "        # zeros. As we have seen in the lecture, the shape of scale and shift    #\n",
        "        # parameters remains the same as in Batchnorm, initialize these parameters\n",
        "        # with appropriate dimensions. Dont forget to encapsulate these scale and#\n",
        "        # shift initializations with nn.Parameter                                #\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Initialize scale (gamma) to ones and shift (beta) to zeros\n",
        "        self.gamma = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.beta = nn.Parameter(torch.zeros(emb_dim))\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        \"\"\"\n",
        "        An implementation of the forward pass of the Layer Normalization.\n",
        "\n",
        "        args:\n",
        "            x: a Tensor of shape (N, K, M) or (N, K) where N is the batch size, K\n",
        "                is the sequence length and M is the embedding dimension\n",
        "\n",
        "        returns:\n",
        "            y: a Tensor of shape (N, K, M) or (N, K) after applying layer\n",
        "                normalization\n",
        "\n",
        "        \"\"\"\n",
        "        y = None\n",
        "        ##########################################################################\n",
        "        # TODO: Implement the forward pass of the LayerNormalization layer.      #\n",
        "        # Compute the mean and standard deviation of input and use these to      #\n",
        "        # normalize the input. Further, use self.gamma and self.beta to scale    #\n",
        "        # these and shift this normalized input. Don't use torch.std to compute  #\n",
        "        # the standard deviation.                                                #\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        variance = ((x - mean) ** 2).mean(dim=-1, keepdim=True)\n",
        "\n",
        "        # Normalize the input\n",
        "        x_normalized = (x - mean) / torch.sqrt(variance + self.epsilon)\n",
        "\n",
        "        # Apply scale (gamma) and shift (beta)\n",
        "        y = self.gamma * x_normalized + self.beta\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdf95ed8",
      "metadata": {
        "id": "cdf95ed8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e7985a-3e95-4eb4-f613-79357a363271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LayerNormalization error:  1.3772273765080196e-06\n",
            "LayerNormalization grad error:  2.2087854796632033e-07\n"
          ]
        }
      ],
      "source": [
        "reset_seed(0)\n",
        "N = 2\n",
        "K = 4\n",
        "norm = LayerNormalization(K)\n",
        "inp = torch.linspace(-0.4, 0.6, steps=N * K, requires_grad=True).reshape(N, K)\n",
        "\n",
        "inp.retain_grad()\n",
        "y = norm(inp)\n",
        "\n",
        "y_expected = torch.tensor(\n",
        "    [[-1.34164, -0.44721, 0.44721, 1.34164], [-1.34164, -0.44721, 0.44721, 1.34164]]\n",
        ")\n",
        "\n",
        "dy_expected = torch.tensor(\n",
        "    [[  5.70524,  -2.77289, -11.56993,   8.63758],\n",
        "        [  2.26242,  -4.44330,   2.09933,   0.08154]]\n",
        ")\n",
        "\n",
        "dy = torch.randn(*y.shape)\n",
        "y.backward(dy)\n",
        "inp_grad = inp.grad\n",
        "\n",
        "print(\"LayerNormalization error: \", rel_error(y_expected, y))\n",
        "print(\"LayerNormalization grad error: \", rel_error(dy_expected, inp_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e86954f-f7df-4d7f-ab6c-2fe1dcb1f5b0",
      "metadata": {
        "id": "5e86954f-f7df-4d7f-ab6c-2fe1dcb1f5b0"
      },
      "source": [
        "### FeedForward Block"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3404d48c-95ce-47d6-ae27-93590552e221",
      "metadata": {
        "id": "3404d48c-95ce-47d6-ae27-93590552e221"
      },
      "source": [
        "In the image below we have highlighted the parts where FeedForward Block is used.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1WCNACnI-Q6OfU3ngjIMCbNzb1sbFnCgP\" alt=\"Layer_norm\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d49a145",
      "metadata": {
        "id": "5d49a145"
      },
      "source": [
        "Next, we will implement the `Feedforward` block. These are used in both the Encoder and Decoder network of the Transformer and they consist of stacked MLP and ReLU layers. In the overall architecture, the output of `MultiHeadAttention` is fed into the `FeedForward` block. Implement the `FeedForwardBlock` inside `transformers.py` and execute the following cells to check your implementation. You should expect the errors below 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cc2118cc",
      "metadata": {
        "id": "cc2118cc"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, inp_dim: int, hidden_dim_feedforward: int):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        An implementation of the FeedForward block in the Transformers. We pass\n",
        "        the input through stacked 2 MLPs and 1 ReLU layer. The forward pass has\n",
        "        following architecture:\n",
        "\n",
        "        linear - relu -linear\n",
        "\n",
        "        The input will have a shape of (N, K, M) where N is the batch size, K is\n",
        "        the sequence length and M is the embedding dimension.\n",
        "\n",
        "        args:\n",
        "            inp_dim: int representing embedding dimension of the input tensor\n",
        "\n",
        "            hidden_dim_feedforward: int representing the hidden dimension for\n",
        "                the feedforward block\n",
        "        \"\"\"\n",
        "\n",
        "        ##########################################################################\n",
        "        # TODO: initialize two MLPs here with the first one using inp_dim as input\n",
        "        # dimension and hidden_dim_feedforward as output and the second with     #\n",
        "        # hidden_dim_feedforward as input. You should figure out the output      #\n",
        "        # dimesion of the second MLP. Initialize the weights of all the MLPs     #\n",
        "        # according to the strategy mentioned in SelfAttention block             #\n",
        "        # HINT: Will the shape of input and output shape of the FeedForwardBlock #\n",
        "        # change?                                                                #\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        # First Linear layer: maps `inp_dim` to `hidden_dim_feedforward`\n",
        "        self.fc1 = nn.Linear(inp_dim, hidden_dim_feedforward)\n",
        "\n",
        "        # Second Linear layer: maps `hidden_dim_feedforward` back to `inp_dim`\n",
        "        self.fc2 = nn.Linear(hidden_dim_feedforward, inp_dim)\n",
        "\n",
        "        # Initialize weights as in `SelfAttention`\n",
        "        for layer in [self.fc1, self.fc2]:\n",
        "            D_in, D_out = layer.in_features, layer.out_features\n",
        "            bound = (6 / (D_in + D_out))**0.5\n",
        "            nn.init.uniform_(layer.weight, -bound, bound)\n",
        "            if layer.bias is not None:\n",
        "                nn.init.uniform_(layer.bias, -bound, bound)\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        An implementation of the forward pass of the FeedForward block.\n",
        "\n",
        "        args:\n",
        "            x: a Tensor of shape (N, K, M) which is the output of\n",
        "               MultiHeadAttention\n",
        "        returns:\n",
        "            y: a Tensor of shape (N, K, M)\n",
        "        \"\"\"\n",
        "        y = None\n",
        "        ###########################################################################\n",
        "        # TODO: Use the two MLP layers initialized in the init function to perform#\n",
        "        # a forward pass. You should be using a ReLU layer after the first MLP and#\n",
        "        # no activation after the second MLP                                      #\n",
        "        ###########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        h=self.fc1(x)\n",
        "        m = nn.ReLU()\n",
        "        h1=m(h)\n",
        "        y=self.fc2(h1)\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "764da0f7",
      "metadata": {
        "id": "764da0f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eafef61f-b60a-49ae-8a2b-d7ea476af4a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForwardBlock error:  2.1976866936034156e-07\n",
            "FeedForwardBlock error:  1.0\n"
          ]
        }
      ],
      "source": [
        "reset_seed(0)\n",
        "N = 2\n",
        "K = 4\n",
        "M = emb_size = 4\n",
        "\n",
        "ff_block = FeedForwardBlock(emb_size, 2 * emb_size)\n",
        "\n",
        "for k, v in ff_block.named_parameters():\n",
        "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
        "\n",
        "inp = torch.linspace(-0.4, 0.6, steps=N * K, requires_grad=True).reshape(\n",
        "    N, K\n",
        ")\n",
        "inp.retain_grad()\n",
        "y = ff_block(inp)\n",
        "\n",
        "y_expected = torch.tensor(\n",
        "    [[-2.46161, -0.71662, 1.02838, 2.77337], [-7.56084, -1.69557, 4.16970, 10.03497]]\n",
        ")\n",
        "\n",
        "dy_expected = torch.tensor(\n",
        "    [[0.55105, 0.68884, 0.82662, 0.96441], [0.30734, 0.31821, 0.32908, 0.33996]]\n",
        ")\n",
        "\n",
        "dy = torch.randn(*y.shape)\n",
        "y.backward(dy)\n",
        "inp_grad = inp.grad\n",
        "\n",
        "print(\"FeedForwardBlock error: \", rel_error(y_expected, y))\n",
        "print(\"FeedForwardBlock error: \", rel_error(dy_expected, inp_grad))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09489f50",
      "metadata": {
        "id": "09489f50"
      },
      "source": [
        "Now, if you look back to the original paper, Attention is all you Need, then, we are almost done with the building blocks of a transformer. What's left is:\n",
        "\n",
        "- Encapsulating the building blocks into Encoder Block\n",
        "- Encapsulating the building blocks into Decoder Block\n",
        "- Handling the input data preprocessing and positional encoding.\n",
        "\n",
        "We will first look at implementing the Encoder Block and Decoder block. The positional encoding is a non learnable embedding and we can treat it as a preprocessing step in our DataLoader."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccaf4c99-49ba-49d0-970a-a8ee0ce3656e",
      "metadata": {
        "id": "ccaf4c99-49ba-49d0-970a-a8ee0ce3656e"
      },
      "source": [
        "In the figure below we have highlighted the encoder block in a Transformer. Notice that it is build using all the components we already implemented before. We just have to be careful about\n",
        "the residual connections in various blocks.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1LsTN1BapktFzSo0smWV881kKeeJRfAa_\" alt=\"Layer_norm\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1364689-db62-44f4-85ca-ed005a357f54",
      "metadata": {
        "id": "c1364689-db62-44f4-85ca-ed005a357f54"
      },
      "source": [
        "As shown in the figure above, the encoder block takes it inputs three tensors. We will assume that we have those three tensors, query, key, and value. Run the cell below to check your implementation of the EncoderBlock. You should expect the errors below 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b9fa5fea",
      "metadata": {
        "id": "b9fa5fea"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, num_heads: int, emb_dim: int, feedforward_dim: int, dropout: float\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        This class implements the encoder block for the Transformer model, the\n",
        "        original paper used 6 of these blocks sequentially to train the final model.\n",
        "        Here, we will first initialize the required layers using the building\n",
        "        blocks we have already  implemented, and then finally write the forward\n",
        "        pass using these initialized layers, residual connections and dropouts.\n",
        "\n",
        "        As shown in the Figure 1 of the paper attention is all you need\n",
        "        https://arxiv.org/pdf/1706.03762.pdf, the encoder consists of four components:\n",
        "\n",
        "        1. MultiHead Attention\n",
        "        2. FeedForward layer\n",
        "        3. Residual connections after MultiHead Attention and feedforward layer\n",
        "        4. LayerNorm\n",
        "\n",
        "        The architecture is as follows:\n",
        "\n",
        "       inp - multi_head_attention - out1 - layer_norm(out1 + inp) - dropout - out2 \\\n",
        "        - feedforward - out3 - layer_norm(out3 + out2) - dropout - out\n",
        "\n",
        "        Here, inp is input of the MultiHead Attention of shape (N, K, M), out1,\n",
        "        out2 and out3 are the outputs of the corresponding layers and we add these\n",
        "        outputs to their respective inputs for implementing residual connections.\n",
        "\n",
        "        args:\n",
        "            num_heads: int value specifying the number of heads in the\n",
        "                MultiHeadAttention block of the encoder\n",
        "\n",
        "            emb_dim: int value specifying the embedding dimension of the input\n",
        "                sequence\n",
        "\n",
        "            feedforward_dim: int value specifying the number of hidden units in the\n",
        "                FeedForward layer of Transformer\n",
        "\n",
        "            dropout: float value specifying the dropout value\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        if emb_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"\"\"The value emb_dim = {emb_dim} is not divisible\n",
        "                             by num_heads = {num_heads}. Please select an\n",
        "                             appropriate value.\"\"\"\n",
        "            )\n",
        "\n",
        "        ##########################################################################\n",
        "        # TODO: Initialize the following layers:                                 #\n",
        "        # 1. One MultiHead Attention block using num_heads as number of heads and#\n",
        "        #    emb_dim as the input dimension. You should also be able to compute  #\n",
        "        #    the output dimension of MultiheadHead attention given num_heads and #\n",
        "        #    emb_dim.                                                            #\n",
        "        #    Hint: use the logic that you concatenate the output from each       #\n",
        "        #    SingleHeadAttention inside the MultiHead Attention block and choose #\n",
        "        #    the output dimension such that the concatenated tensor and the input#\n",
        "        #    tensor have the same embedding dimension.                           #\n",
        "        #                                                                        #\n",
        "        # 2. Two LayerNorm layers with input dimension equal to emb_dim          #\n",
        "        # 3. One feedForward block taking input as emb_dim and hidden units as   #\n",
        "        #    feedforward_dim                                                     #\n",
        "        # 4. A Dropout layer with given dropout parameter                        #\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        self.mha=MultiHeadAttention(num_heads,emb_dim,emb_dim)\n",
        "        self.ln1=LayerNormalization(emb_dim)\n",
        "        self.do1=nn.Dropout(p=dropout)\n",
        "        self.ff=FeedForwardBlock(emb_dim,feedforward_dim)\n",
        "        self.ln2=LayerNormalization(emb_dim)\n",
        "        self.do2=nn.Dropout(p=dropout)\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        An implementation of the forward pass of the EncoderBlock of the\n",
        "        Transformer model.\n",
        "        args:\n",
        "            x: a Tensor of shape (N, K, M) as input sequence\n",
        "        returns:\n",
        "            y: a Tensor of shape (N, K, M) as the output of the forward pass\n",
        "        \"\"\"\n",
        "        y = None\n",
        "        ##########################################################################\n",
        "        # TODO: Use the layer initialized in the init function to complete the   #\n",
        "        # forward pass. As Multihead Attention takes in 3 inputs, use the same   #\n",
        "        # input thrice as the input. Follow the Figure 1 in Attention is All you #\n",
        "        # Need paper to complete the rest of the forward pass. You can also take #\n",
        "        # reference from the architecture written in the fucntion documentation. #\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        out1=self.mha(x,x,x)\n",
        "        out2=self.do1(self.ln1(out1+x))\n",
        "        out3=self.ff(out2)\n",
        "        y=self.do2(self.ln2(out3+x))\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "76e8ccb0",
      "metadata": {
        "id": "76e8ccb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c82349-7e49-4934-9f6f-375a1c8f6cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EncoderBlock error 1:  0.5065144936104236\n",
            "EncoderBlock error 2:  0.49392045491119596\n"
          ]
        }
      ],
      "source": [
        "reset_seed(0)\n",
        "N = 2\n",
        "num_heads = 2\n",
        "emb_dim = K = 4\n",
        "feedforward_dim = 8\n",
        "M = inp_emb_size = 4\n",
        "out_emb_size = 8\n",
        "dropout = 0.2\n",
        "\n",
        "enc_seq_inp = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
        "    N, K, M\n",
        ")  # **to_double_cuda\n",
        "\n",
        "enc_block = EncoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
        "\n",
        "for k, v in enc_block.named_parameters():\n",
        "    # print(k, v.shape) # uncomment this to see the weight shape\n",
        "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
        "\n",
        "encoder_out1_expected = torch.tensor(\n",
        "    [[[ 0.00000, -0.31357,  0.69126,  0.00000],\n",
        "         [ 0.42630, -0.25859,  0.72412,  3.87013],\n",
        "         [ 0.00000, -0.31357,  0.69126,  3.89884],\n",
        "         [ 0.47986, -0.30568,  0.69082,  3.90563]],\n",
        "\n",
        "        [[ 0.00000, -0.31641,  0.69000,  3.89921],\n",
        "         [ 0.47986, -0.30568,  0.69082,  3.90563],\n",
        "         [ 0.47986, -0.30568,  0.69082,  3.90563],\n",
        "         [ 0.51781, -0.30853,  0.71598,  3.85171]]]\n",
        ")\n",
        "encoder_out1 = enc_block(enc_seq_inp)\n",
        "print(\"EncoderBlock error 1: \", rel_error(encoder_out1, encoder_out1_expected))\n",
        "\n",
        "\n",
        "N = 2\n",
        "num_heads = 1\n",
        "emb_dim = K = 4\n",
        "feedforward_dim = 8\n",
        "M = inp_emb_size = 4\n",
        "out_emb_size = 8\n",
        "dropout = 0.2\n",
        "\n",
        "enc_seq_inp = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
        "    N, K, M\n",
        ")  # **to_double_cuda\n",
        "\n",
        "enc_block = EncoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
        "\n",
        "for k, v in enc_block.named_parameters():\n",
        "    # print(k, v.shape) # uncomment this to see the weight shape\n",
        "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
        "\n",
        "encoder_out2_expected = torch.tensor(\n",
        "    [[[ 0.42630, -0.00000,  0.72412,  3.87013],\n",
        "         [ 0.49614, -0.31357,  0.00000,  3.89884],\n",
        "         [ 0.47986, -0.30568,  0.69082,  0.00000],\n",
        "         [ 0.51654, -0.32455,  0.69035,  3.89216]],\n",
        "\n",
        "        [[ 0.47986, -0.30568,  0.69082,  0.00000],\n",
        "         [ 0.49614, -0.31357,  0.69126,  3.89884],\n",
        "         [ 0.00000, -0.30354,  0.76272,  3.75311],\n",
        "         [ 0.49614, -0.31357,  0.69126,  3.89884]]]\n",
        ")\n",
        "encoder_out2 = enc_block(enc_seq_inp)\n",
        "print(\"EncoderBlock error 2: \", rel_error(encoder_out2, encoder_out2_expected))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc029c5b-ca52-454a-b8a4-b07cea708b5e",
      "metadata": {
        "id": "cc029c5b-ca52-454a-b8a4-b07cea708b5e"
      },
      "source": [
        "Great! You're almost done with the implementation of the Transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b2616ef-5934-4a50-8f51-642ef635e2cb",
      "metadata": {
        "id": "4b2616ef-5934-4a50-8f51-642ef635e2cb"
      },
      "source": [
        "### Decoder Block\n",
        "\n",
        "The image below shows the highlighted Decoder block. Notice how it takes the input from the encoder and the target sequence.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1DwU3BJsA0mUWTWlXNtNolB4oc5K4Z9PE\" alt=\"Layer_norm\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e2d5d7-6918-4ea8-a369-0d53304ff1a7",
      "metadata": {
        "id": "19e2d5d7-6918-4ea8-a369-0d53304ff1a7"
      },
      "source": [
        "Now, we will look at the implementation of the decoder. In the  class we learned about encoder only model that can be used for tasks like sequence classification but for more complicated tasks like sequence to sequence we need a decoder network that can transform the output of the encoder to a target sequence. This kind of architecture is important in tasks like language translation where we have a sequence as input and a sequence as output. This decoder takes the input from the encoder and the previous generated value to generate the next value. During training, we use a Mask on the input so that the decoder network can't look ahead in the future and during inference we sequentially process the data.\n",
        "\n",
        "Before moving to implementing the Decoder Block, we should pay attention to the figure above. It says a \"Masked MultiHead Attention\" which actually prevents the decoder from looking ahead into the future. Lets understand with an example here. We have an expression as `BOS POSITIVE 01 add POSITIVE 00 EOS`, i.e. `1+0` that gives output as `BOS POSITIVE 01 EOS`, i.e. `+1`. Lets focus on the output sequence here. This is a sequence of length 5 (after applying our preprocessing code) and will will get transformed into *key*, *query*, and *value* matrix of dimension $5\\times128$, $5\\times128$ and $5\\times128$ respectively, where 128 is the embedding dimension of the Transformer. Now, while training, we input these vectors in the `self_attention_no_loop_batch` without mask. It will compute the dot product between *query* and *key* to generate a $5\\times5$ matrix where the first row (shape $1\\times5$) of that matrix tells us how much the word `EOS` is related with `EOS`, `POSITIVE`, `0`, `1`, and `EOS`. This means that it will use the weights of all these tokens to learn the final sequence that is to be predicted. This is okay when we are training the model but what happens when we perform inference? We start with a brand new expression, input this expression in the encoder but this time we only have the first starting token `EOS` for decoder and we don't know about the rest of the tokens in the sequence. Hence, a solution to this problem is to mask the weights inside the function `self_attention_no_loop_batch` for only the decoder part. This masking should prevent the decoder from accessing the future or next elements.\n",
        "\n",
        "We will now look at how to generate this mask for a given sequence. Then, you should also update the `self_attention_no_loop_batch` to use the mask variable appropriately. Implement the `get_subsequent_mask`, `self_attention_no_loop_batch` with mask inside `transformers.py` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "39392320-f75a-49cc-9d90-364d5eab897b",
      "metadata": {
        "id": "39392320-f75a-49cc-9d90-364d5eab897b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6427d0-75c9-46e8-caa9-c218592ceacf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_subsequent_mask error:  0.0\n"
          ]
        }
      ],
      "source": [
        "def get_subsequent_mask(seq):\n",
        "    \"\"\"\n",
        "    An implementation of the decoder self attention mask. This will be used to\n",
        "    mask the target sequence while training the model. The input shape here is\n",
        "    (N, K) where N is the batch size and K is the sequence length.\n",
        "\n",
        "    args:\n",
        "        seq: a tensor of shape (N, K) where N is the batch sieze and K is the\n",
        "             length of the sequence\n",
        "    return:\n",
        "        mask: a tensor of shape (N, K, K) where N is the batch sieze and K is the\n",
        "              length of the sequence\n",
        "\n",
        "    Given a sequence of length K, we want to mask the weights inside the function\n",
        "    `self_attention_no_loop_batch` so that it prohibits the decoder to look ahead\n",
        "    in the future\n",
        "    \"\"\"\n",
        "    mask = None\n",
        "    ###############################################################################\n",
        "    # TODO: This function constructs mask for the decoder part of the Transformer.#\n",
        "    # To implement this, for each sequence (of K) in the batch(N) return a        #\n",
        "    # boolean matrix that is True for the place where we have to apply mask and   #\n",
        "    # False where we don't have to apply the mask.                                #\n",
        "    #                                                                             #\n",
        "    ###############################################################################\n",
        "    size = seq.size(1)  # Get the sequence length (K)\n",
        "\n",
        "    # Create a matrix with shape (K, K) where elements in the upper triangular part are True (to mask)\n",
        "    subsequent_mask = torch.triu(torch.ones((size, size), dtype=torch.bool), diagonal=1)\n",
        "    return subsequent_mask\n",
        "\n",
        "reset_seed(0)\n",
        "seq_len_enc = K = 4\n",
        "M = inp_emb_size = 3\n",
        "\n",
        "inp_sequence = torch.linspace(-0.4, 0.6, steps=K * M, requires_grad=True).reshape(\n",
        "    K, M\n",
        ")  # **to_double_cuda\n",
        "\n",
        "mask_expected = torch.tensor(\n",
        "    [\n",
        "        [[False, True, True], [False, False, True], [False, False, False]],\n",
        "        [[False, True, True], [False, False, True], [False, False, False]],\n",
        "        [[False, True, True], [False, False, True], [False, False, False]],\n",
        "        [[False, True, True], [False, False, True], [False, False, False]],\n",
        "    ]\n",
        ")\n",
        "mask_predicted = get_subsequent_mask(inp_sequence)\n",
        "print(\n",
        "    \"get_subsequent_mask error: \", rel_error(mask_predicted.int(), mask_expected.int())\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408e7333-9d14-498d-8661-b9f8703b24fe",
      "metadata": {
        "id": "408e7333-9d14-498d-8661-b9f8703b24fe"
      },
      "outputs": [],
      "source": [
        "from transformers import scaled_dot_product_no_loop_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b6d6114-f8fa-484b-8084-ea078db1545a",
      "metadata": {
        "id": "0b6d6114-f8fa-484b-8084-ea078db1545a"
      },
      "outputs": [],
      "source": [
        "reset_seed(0)\n",
        "N = 4\n",
        "K = 3\n",
        "M = 3\n",
        "\n",
        "query = torch.linspace(-0.4, 0.6, steps=K * M * N, requires_grad=True).reshape(N, K, M)\n",
        "key = torch.linspace(-0.1, 0.2, steps=K * M * N, requires_grad=True).reshape(N, K, M)\n",
        "value = torch.linspace(0.4, 0.8, steps=K * M * N, requires_grad=True).reshape(N, K, M)\n",
        "\n",
        "y_expected = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [0.40000, 0.41143, 0.42286],\n",
        "            [0.41703, 0.42846, 0.43989],\n",
        "            [0.43408, 0.44551, 0.45694],\n",
        "        ],\n",
        "        [\n",
        "            [0.50286, 0.51429, 0.52571],\n",
        "            [0.51999, 0.53142, 0.54285],\n",
        "            [0.53720, 0.54863, 0.56006],\n",
        "        ],\n",
        "        [\n",
        "            [0.60571, 0.61714, 0.62857],\n",
        "            [0.62294, 0.63437, 0.64580],\n",
        "            [0.64032, 0.65175, 0.66318],\n",
        "        ],\n",
        "        [\n",
        "            [0.70857, 0.72000, 0.73143],\n",
        "            [0.72590, 0.73733, 0.74876],\n",
        "            [0.74344, 0.75487, 0.76630],\n",
        "        ],\n",
        "    ]\n",
        ")\n",
        "y_predicted, _ = scaled_dot_product_no_loop_batch(query, key, value, mask_expected)\n",
        "\n",
        "print(\"scaled_dot_product_no_loop_batch error: \", rel_error(y_expected, y_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9671dba4-bf00-434d-865d-97c967e154a6",
      "metadata": {
        "id": "9671dba4-bf00-434d-865d-97c967e154a6"
      },
      "source": [
        "Lets finally implement the decoder block now that we have all the required tools to implement it. Fill in the init function and the forward pass of the `DecoderBlock` inside `transformers.py`. Run the following cells to check your implementation of the `DecoderBlock`. You should expect the errors below 1e-5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "aa350b69",
      "metadata": {
        "id": "aa350b69"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, num_heads: int, emb_dim: int, feedforward_dim: int, dropout: float\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if emb_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"\"\"The value emb_dim = {emb_dim} is not divisible\n",
        "                             by num_heads = {num_heads}. Please select an\n",
        "                             appropriate value.\"\"\"\n",
        "            )\n",
        "\n",
        "        \"\"\"\n",
        "        The function implements the DecoderBlock for the Transformer model. In the\n",
        "        class we learned about encoder only model that can be used for tasks like\n",
        "        sequence classification but for more complicated tasks like sequence to\n",
        "        sequence we need a decoder network that can transformt the output of the\n",
        "        encoder to a target sequence. This kind of architecture is important in\n",
        "        tasks like language translation where we have a sequence as input and a\n",
        "        sequence as output.\n",
        "\n",
        "        As shown in the Figure 1 of the paper attention is all you need\n",
        "        https://arxiv.org/pdf/1706.03762.pdf, the encoder consists of 5 components:\n",
        "\n",
        "        1. Masked MultiHead Attention\n",
        "        2. MultiHead Attention\n",
        "        3. FeedForward layer\n",
        "        4. Residual connections after MultiHead Attention and feedforward layer\n",
        "        5. LayerNorm\n",
        "\n",
        "        The Masked MultiHead Attention takes the target, masks it as per the\n",
        "        function get_subsequent_mask and then gives the output as per the MultiHead\n",
        "        Attention layer. Further, another Multihead Attention block here takes the\n",
        "        encoder output and the output from Masked Multihead Attention layer giving\n",
        "        the output that helps the model create interaction between input and\n",
        "        targets. As this block helps in interation of the input and target, it\n",
        "        is also sometimes called the cross attention.\n",
        "\n",
        "        The architecture is as follows:\n",
        "\n",
        "        inp - masked_multi_head_attention - out1 - layer_norm(inp + out1) - \\\n",
        "        dropout - (out2 and enc_out) -  multi_head_attention - out3 - \\\n",
        "        layer_norm(out3 + out2) - dropout - out4 - feed_forward - out5 - \\\n",
        "        layer_norm(out5 + out4) - dropout - out\n",
        "\n",
        "        Here, out1, out2, out3, out4, out5 are the corresponding outputs for the\n",
        "        layers, enc_out is the encoder output and we add these outputs to their\n",
        "        respective inputs for implementing residual connections.\n",
        "\n",
        "        args:\n",
        "            num_heads: int value representing number of heads\n",
        "\n",
        "            emb_dim: int value representing embedding dimension\n",
        "\n",
        "            feedforward_dim: int representing hidden layers in the feed forward\n",
        "                model\n",
        "\n",
        "            dropout: float representing the dropout value\n",
        "        \"\"\"\n",
        "        self.attention_self = None\n",
        "        self.attention_cross = None\n",
        "        self.feed_forward = None\n",
        "        self.norm1 = None\n",
        "        self.norm2 = None\n",
        "        self.norm3 = None\n",
        "        self.dropout = None\n",
        "        self.feed_forward = None\n",
        "        ##########################################################################\n",
        "        # TODO: Initialize the following layers:                                 #\n",
        "        # 1. Two MultiheadAttention layers with num_heads number of heads, emb_dim\n",
        "        #     as the embedding dimension. As done in Encoder, you should be able to\n",
        "        #     figure out the output dimension of both the MultiHeadAttention.    #\n",
        "        # 2. One FeedForward block that takes in emb_dim as input dimension and  #\n",
        "        #   feedforward_dim as hidden layers                                     #\n",
        "        # 3. LayerNormalization layers after each of the block                   #\n",
        "        # 4. Dropout after each of the block                                     #\n",
        "        ##########################################################################\n",
        "\n",
        "        # Replace \"pass\" statement with your code\n",
        "\n",
        "        self.attention_self=MultiHeadAttention(num_heads,emb_dim,emb_dim)\n",
        "        self.norm1=LayerNormalization(emb_dim)\n",
        "        self.dropout=nn.Dropout(p=dropout)\n",
        "        self.attention_cross=MultiHeadAttention(num_heads,emb_dim,emb_dim)\n",
        "        self.norm2=LayerNormalization(emb_dim)\n",
        "        self.dropout=nn.Dropout(p=dropout)\n",
        "        self.feed_forward=FeedForwardBlock(emb_dim,feedforward_dim)\n",
        "        self.norm3=LayerNormalization(emb_dim)\n",
        "        self.dropout=nn.Dropout(p=dropout)\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "\n",
        "    def forward(\n",
        "        self, dec_inp: Tensor, enc_inp: Tensor, mask: Tensor = None\n",
        "    ) -> Tensor:\n",
        "\n",
        "        \"\"\"\n",
        "        args:\n",
        "            dec_inp: a Tensor of shape (N, K, M)\n",
        "            enc_inp: a Tensor of shape (N, K, M)\n",
        "            mask: a Tensor of shape (N, K, K)\n",
        "\n",
        "        This function will handle the forward pass of the Decoder block. It takes\n",
        "        in input as enc_inp which is the encoder output and a tensor dec_inp which\n",
        "        is the target sequence shifted by one in case of training and an initial\n",
        "        token \"BOS\" during inference\n",
        "        \"\"\"\n",
        "        y = None\n",
        "        ##########################################################################\n",
        "        # TODO: Using the layers initialized in the init function, implement the #\n",
        "        # forward pass of the decoder block. Pass the dec_inp to the             #\n",
        "        # self.attention_self layer. This layer is responsible for the self      #\n",
        "        # interation of the decoder input. You should follow the Figure 1 in     #\n",
        "        # Attention is All you need paper to implenment the rest of the forward  #\n",
        "        # pass. Don't forget to apply the residual connections for different layers.\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        out1=self.attention_self(dec_inp,dec_inp,dec_inp,mask)\n",
        "        out2=self.dropout(self.norm1(out1+dec_inp))\n",
        "        out3=self.attention_cross(out2,enc_inp,enc_inp)\n",
        "        out4=self.dropout(self.norm2(out3+out2))\n",
        "        out5=self.feed_forward(out4)\n",
        "        y=self.dropout(self.norm3(out5+out4))\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "62927d47",
      "metadata": {
        "id": "62927d47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c03b0d-e9c9-422c-b191-9264d034b10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecoderBlock error:  0.5019683888273199\n",
            "DecoderBlock error:  0.49699180712256213\n"
          ]
        }
      ],
      "source": [
        "reset_seed(0)\n",
        "N = 2\n",
        "num_heads = 2\n",
        "seq_len_enc = K1 = 4\n",
        "seq_len_dec = K2 = 2\n",
        "feedforward_dim = 8\n",
        "M = emb_dim = 4\n",
        "out_emb_size = 8\n",
        "dropout = 0.2\n",
        "\n",
        "dec_inp = torch.linspace(-0.4, 0.6, steps=N * K1 * M, requires_grad=True).reshape(\n",
        "    N, K1, M\n",
        ")\n",
        "enc_out = torch.linspace(-0.4, 0.6, steps=N * K2 * M, requires_grad=True).reshape(\n",
        "    N, K2, M\n",
        ")\n",
        "dec_block = DecoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
        "\n",
        "for k, v in dec_block.named_parameters():\n",
        "    # print(k, v.shape) # uncomment this to see the weight shape\n",
        "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
        "\n",
        "\n",
        "dec_out_expected = torch.tensor(\n",
        "    [[[ 0.50623, -0.32496,  0.00000,  0.00000],\n",
        "         [ 0.00000, -0.31690,  0.76956,  3.72647],\n",
        "         [ 0.49014, -0.32809,  0.66595,  3.93773],\n",
        "         [ 0.00000, -0.00000,  0.68203,  3.90856]],\n",
        "\n",
        "        [[ 0.51042, -0.32787,  0.68093,  3.90848],\n",
        "         [ 0.00000, -0.31637,  0.72275,  3.83122],\n",
        "         [ 0.64868, -0.00000,  0.77715,  0.00000],\n",
        "         [ 0.00000, -0.33105,  0.66565,  3.93602]]]\n",
        ")\n",
        "dec_out1 = dec_block(dec_inp, enc_out)\n",
        "print(\"DecoderBlock error: \", rel_error(dec_out1, dec_out_expected))\n",
        "\n",
        "N = 2\n",
        "num_heads = 2\n",
        "seq_len_enc = K1 = 4\n",
        "seq_len_dec = K2 = 4\n",
        "feedforward_dim = 4\n",
        "M = emb_dim = 4\n",
        "out_emb_size = 8\n",
        "dropout = 0.2\n",
        "\n",
        "dec_inp = torch.linspace(-0.4, 0.6, steps=N * K1 * M, requires_grad=True).reshape(\n",
        "    N, K1, M\n",
        ")\n",
        "enc_out = torch.linspace(-0.4, 0.6, steps=N * K2 * M, requires_grad=True).reshape(\n",
        "    N, K2, M\n",
        ")\n",
        "dec_block = DecoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
        "\n",
        "for k, v in dec_block.named_parameters():\n",
        "    # print(k, v.shape) # uncomment this to see the weight shape\n",
        "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
        "\n",
        "\n",
        "dec_out_expected = torch.tensor(\n",
        "    [[[ 0.46707, -0.31916,  0.66218,  3.95182],\n",
        "         [ 0.00000, -0.31116,  0.66325,  0.00000],\n",
        "         [ 0.44538, -0.32419,  0.64068,  3.98847],\n",
        "         [ 0.49012, -0.31276,  0.68795,  3.90610]],\n",
        "\n",
        "        [[ 0.45800, -0.33023,  0.64106,  3.98324],\n",
        "         [ 0.45829, -0.31487,  0.66203,  3.95529],\n",
        "         [ 0.59787, -0.00000,  0.72361,  0.00000],\n",
        "         [ 0.70958, -0.37051,  0.78886,  3.63179]]]\n",
        ")\n",
        "dec_out2 = dec_block(dec_inp, enc_out)\n",
        "print(\"DecoderBlock error: \", rel_error(dec_out2, dec_out_expected))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cba0c23d-ba32-442d-87fe-26c1f9848aad",
      "metadata": {
        "id": "cba0c23d-ba32-442d-87fe-26c1f9848aad"
      },
      "source": [
        "Based on the implementation of `EncoderBlock` and `DecoderBlock`, we have implemented the `Encoder` and `Decoder` networks for you in transformers.py. You should be able to understand the input and outputs of these Encoder and Decoder blocks. Implement the Transformer block inside transformer.py using these networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20a41e64-4de5-4289-be47-3e7282e88f35",
      "metadata": {
        "id": "20a41e64-4de5-4289-be47-3e7282e88f35"
      },
      "source": [
        "## Part III: Data loader\n",
        "\n",
        "In this part, we will have a look at creating the final data loader for the task, that can be used to train the Transformer model. This will comprise of two things:\n",
        "\n",
        "- Implement Positional Encoding\n",
        "- Create a dataloader using the `prepocess_input_sequence` fucntion that we created in Part I."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1249f651-4de3-4dd7-aa85-7d8bfb3e1e4f",
      "metadata": {
        "id": "1249f651-4de3-4dd7-aa85-7d8bfb3e1e4f"
      },
      "source": [
        "Lets start with implementing the Positional Encoding for the input. The positional encodings make the Transformers positionally aware about sequences. These are usually added to the input and hence should be same shape as input. As these are not learnable, they remain constant throughtout the training process. For this reason, we can look at it as a pre-processing step that's done on the input. Our strategy here would be to implement positional encoding function and use it later while creating DataLoader for the toy dataset.\n",
        "\n",
        "Lets look at the simplest kind of positional encoding, i.e. for a sequence of length K, assign the nth element in the sequence a value of n/K, where n starts from 0. Implement the position_encoding_simple inside `transformers.py`. You should expect error less than 1e-9 here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0798f4f1-fdb5-4e3b-9dba-5e1e1d64277b",
      "metadata": {
        "id": "0798f4f1-fdb5-4e3b-9dba-5e1e1d64277b"
      },
      "source": [
        "### Simple positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "90e1dad2-aecb-487e-8ece-c23b7e48c291",
      "metadata": {
        "id": "90e1dad2-aecb-487e-8ece-c23b7e48c291",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c77af0-0a19-42db-e82d-2c60ebddbca3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "position_encoding_simple error:  0.14285714285714285\n",
            "position_encoding_simple error:  0.11111110743181195\n"
          ]
        }
      ],
      "source": [
        "def position_encoding_simple(K: int, M: int) -> Tensor:\n",
        "    \"\"\"\n",
        "    An implementation of the simple positional encoding using uniform intervals\n",
        "    for a sequence.\n",
        "\n",
        "    args:\n",
        "        K: int representing sequence length\n",
        "        M: int representing embedding dimension for the sequence\n",
        "\n",
        "    return:\n",
        "        y: a Tensor of shape (1, K, M)\n",
        "    \"\"\"\n",
        "    y = None\n",
        "    ##############################################################################\n",
        "    # TODO: Given the length of input sequence K, construct a 1D Tensor of length#\n",
        "    # K with nth element as n/K, where n starts from 0. Replicate this tensor M  #\n",
        "    # times to create a tensor of the required output shape                      #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    # Create a 1D tensor of length K with elements n/K where n ranges from 0 to K-1\n",
        "    base_encoding = torch.linspace(0, 1, K).unsqueeze(1)\n",
        "\n",
        "    # Replicate the tensor M times along the last dimension to create (K, M)\n",
        "    y = base_encoding.repeat(1, M).unsqueeze(0)  # Shape (1, K, M)\n",
        "    ##############################################################################\n",
        "    #               END OF YOUR CODE                                             #\n",
        "    ##############################################################################\n",
        "    return y\n",
        "\n",
        "reset_seed(0)\n",
        "K = 4\n",
        "M = emb_size = 4\n",
        "\n",
        "y = position_encoding_simple(K, M)\n",
        "y_expected = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [0.00000, 0.00000, 0.00000, 0.00000],\n",
        "            [0.25000, 0.25000, 0.25000, 0.25000],\n",
        "            [0.50000, 0.50000, 0.50000, 0.50000],\n",
        "            [0.75000, 0.75000, 0.75000, 0.75000],\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"position_encoding_simple error: \", rel_error(y, y_expected))\n",
        "\n",
        "K = 5\n",
        "M = emb_size = 3\n",
        "\n",
        "\n",
        "y = position_encoding_simple(K, M)\n",
        "y_expected = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [0.00000, 0.00000, 0.00000],\n",
        "            [0.20000, 0.20000, 0.20000],\n",
        "            [0.40000, 0.40000, 0.40000],\n",
        "            [0.60000, 0.60000, 0.60000],\n",
        "            [0.80000, 0.80000, 0.80000],\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "print(\"position_encoding_simple error: \", rel_error(y, y_expected))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c8ddbbc-1631-4781-849d-3fd27a3b0619",
      "metadata": {
        "id": "8c8ddbbc-1631-4781-849d-3fd27a3b0619"
      },
      "source": [
        "### Sinusoid positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e3db3a0",
      "metadata": {
        "id": "1e3db3a0"
      },
      "source": [
        "Now that we have looked at a simple positional encoding, we can see one major drawback, which is that if the sequence length gets larger, the difference between two consecutive positional encodings becomes smaller and smaller and it in turn defeats a purpose of positional awareness, as there is very small diference in two consecutive positions. Another issue is that for each position we replicated it along embedding dimension, hence introducing redundancy which might not help the network in learning anything new. There could be different tricks that can be used to make a positional encoding that could solve these problems.\n",
        "\n",
        "Lets look at more mature version of a positonal encoding that uses a combination of sines and cosines function, also called sinusoid. This is also the positional encoding used in the original Transformer paper. For each element in the sequence (length K) with position $p$ and embedding (dimension M) positon $i$, we can define the positional encoding as:\n",
        "\n",
        "$$PE_{(p, 2i)} = \\sin\\left(\\frac{p}{10000^a}\\right)$$\n",
        "$$PE_{(p, 2i+1)} = \\cos\\left(\\frac{p}{10000^a}\\right)$$\n",
        "\n",
        "$$\\text{Where }a = \\left\\lfloor{\\frac{2i}{M}}\\right\\rfloor \\text{and M is the Embedding dimension of the Transformer}$$\n",
        "\n",
        "Here, $p$ remains constant for a position in the sequence and we assign alternating sines and cosines along the embedding dimension.\n",
        "\n",
        "Implement the fucntion `position_encoding` inside `transformers.py`. You should expect errors below 1e-5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "135f8367-b90d-4956-8783-1e0c8e3e2c94",
      "metadata": {
        "id": "135f8367-b90d-4956-8783-1e0c8e3e2c94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "fa8ef61b-0935-41e0-f611-3d0796dbb24e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "position_encoding error:  0.9947700500488281\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [5, 1].  Tensor sizes: [5, 2]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-8d6b49f57782>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_encoding_sinusoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m y_expected = torch.tensor(\n\u001b[1;32m     64\u001b[0m     [\n",
            "\u001b[0;32m<ipython-input-32-8d6b49f57782>\u001b[0m in \u001b[0;36mposition_encoding_sinusoid\u001b[0;34m(K, M)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mpe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiv_term\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply sin to even indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mpe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdiv_term\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply cos to odd indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Add a batch dimension at the start to make it (1, K, M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [5, 1].  Tensor sizes: [5, 2]"
          ]
        }
      ],
      "source": [
        "def position_encoding_sinusoid(K: int, M: int) -> Tensor:\n",
        "\n",
        "    \"\"\"\n",
        "    An implementation of the sinousoidal positional encodings.\n",
        "\n",
        "    args:\n",
        "        K: int representing sequence length\n",
        "        M: int representing embedding dimension for the sequence\n",
        "\n",
        "    return:\n",
        "        y: a Tensor of shape (1, K, M)\n",
        "\n",
        "    \"\"\"\n",
        "    y = None\n",
        "    ##############################################################################\n",
        "    # TODO: Given the length of input sequence K and embedding dimension M       #\n",
        "    # construct a tesnor of shape (K, M) where the value along the dimensions    #\n",
        "    # follow the equations given in the notebook. Make sure to keep in mind the  #\n",
        "    # alternating sines and cosines along the embedding dimension M.             #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    # Create a tensor of shape (K, 1) for positions\n",
        "    position = torch.arange(K, dtype=torch.float32).unsqueeze(1)  # Shape (K, 1)\n",
        "\n",
        "    # Compute the scaling factor for each dimension\n",
        "    div_term = torch.exp(torch.arange(0, M, 2).float() * (-torch.log(torch.tensor(10000.0)) / M))\n",
        "\n",
        "    # Compute the sinusoidal encodings\n",
        "    pe = torch.zeros((K, M))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)  # Apply sin to even indices\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)  # Apply cos to odd indices\n",
        "\n",
        "    # Add a batch dimension at the start to make it (1, K, M)\n",
        "    y = pe.unsqueeze(0)\n",
        "    ##############################################################################\n",
        "    #               END OF YOUR CODE                                             #\n",
        "    ##############################################################################\n",
        "    return y\n",
        "\n",
        "reset_seed(0)\n",
        "K = 4\n",
        "M = emb_size = 4\n",
        "\n",
        "y1 = position_encoding_sinusoid(K, M)\n",
        "y_expected = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [0.00000, 1.00000, 0.00000, 1.00000],\n",
        "            [0.84147, 0.54030, 0.84147, 0.54030],\n",
        "            [0.90930, -0.41615, 0.90930, -0.41615],\n",
        "            [0.14112, -0.98999, 0.14112, -0.98999],\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"position_encoding error: \", rel_error(y1, y_expected))\n",
        "\n",
        "K = 5\n",
        "M = emb_size = 3\n",
        "\n",
        "\n",
        "y2 = position_encoding_sinusoid(K, M)\n",
        "y_expected = torch.tensor(\n",
        "    [\n",
        "        [\n",
        "            [0.00000, 1.00000, 0.00000],\n",
        "            [0.84147, 0.54030, 0.84147],\n",
        "            [0.90930, -0.41615, 0.90930],\n",
        "            [0.14112, -0.98999, 0.14112],\n",
        "            [-0.75680, -0.65364, -0.75680],\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "print(\"position_encoding error: \", rel_error(y2, y_expected))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "579e6f2e-d2f0-4a10-a54f-c65ecdccc587",
      "metadata": {
        "id": "579e6f2e-d2f0-4a10-a54f-c65ecdccc587"
      },
      "source": [
        "### Constructing the DataLoader for the toy dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f2d8dd1",
      "metadata": {
        "id": "5f2d8dd1"
      },
      "source": [
        "Now we will use the implemented positonal encodings to construct a DataLoader in Pytorch. The function of a data loader is to return a batch for training/validation. We first make a Dataset class that gives us a single element in the batch and then use a DataLoader to wrap the dataset. We inherit the Dataset from `torch.utils.data.Dataset` class. This class consists of two important functions that you'd change depending on your usecase (for e.g. the upcoming project!). The first function is `__init__`, this consists of the components that are *static*, in other words, these are the variables that won't change when we want the next element from the complete data. The second fucntion is `__getitem__` which contains the core functionality of the final dataloader.\n",
        "\n",
        "To get the final dataloader, we wrap the `train_data` and `test_data` in `torch.utils.data.DataLoader` class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a673ee9f-c5ae-438c-b696-9662dcb87c52",
      "metadata": {
        "id": "a673ee9f-c5ae-438c-b696-9662dcb87c52"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "class AddSubDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_seqs,\n",
        "        target_seqs,\n",
        "        convert_str_to_tokens,\n",
        "        special_tokens,\n",
        "        emb_dim,\n",
        "        pos_encode,\n",
        "    ):\n",
        "\n",
        "        \"\"\"\n",
        "        The class implements the dataloader that will be used for the toy dataset.\n",
        "\n",
        "        args:\n",
        "            input_seqs: A list of input strings\n",
        "            target_seqs: A list of output strings\n",
        "            convert_str_to_tokens: Dictionary to convert input string to tokens\n",
        "            special_tokens: A list of strings\n",
        "            emb_dim: embedding dimension of the transformer\n",
        "            pos_encode: A function to compute positional encoding for the data\n",
        "        \"\"\"\n",
        "\n",
        "        self.input_seqs = input_seqs\n",
        "        self.target_seqs = target_seqs\n",
        "        self.convert_str_to_tokens = convert_str_to_tokens\n",
        "        self.emb_dim = emb_dim\n",
        "        self.special_tokens = special_tokens\n",
        "        self.pos_encode = pos_encode\n",
        "\n",
        "    def preprocess(self, inp):\n",
        "        return prepocess_input_sequence(\n",
        "            inp, self.convert_str_to_tokens, self.special_tokens\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        The core fucntion to get element with index idx in the data.\n",
        "        args:\n",
        "            idx: index of the element that we need to extract from the data\n",
        "        returns:\n",
        "            preprocess_inp: A 1D tensor of length K, where K is the input sequence\n",
        "                length\n",
        "            inp_pos_enc: A tensor of shape (K, M), where K is the sequence length\n",
        "                and M is the embedding dimension\n",
        "            preprocess_out: A 1D tensor of length O, where O is the output\n",
        "                sequence length\n",
        "            out_pos_enc: A tensor of shape (O, M), where O is the sequence length\n",
        "                and M is the embedding dimension\n",
        "        \"\"\"\n",
        "\n",
        "        inp = self.input_seqs[idx]\n",
        "        out = self.target_seqs[idx]\n",
        "        preprocess_inp = torch.tensor(self.preprocess(inp))\n",
        "        preprocess_out = torch.tensor(self.preprocess(out))\n",
        "        inp_pos = len(preprocess_inp)\n",
        "        inp_pos_enc = self.pos_encode(inp_pos, self.emb_dim)\n",
        "        out_pos = len(preprocess_out)\n",
        "        out_pos_enc = self.pos_encode(out_pos, self.emb_dim)\n",
        "\n",
        "        return preprocess_inp, inp_pos_enc[0], preprocess_out, out_pos_enc[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_seqs)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "X, y = data[\"inp_expression\"], data[\"out_expression\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
        "\n",
        "train_data = AddSubDataset(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    convert_str_to_tokens,\n",
        "    SPECIAL_TOKENS,\n",
        "    32,\n",
        "    position_encoding_simple,\n",
        ")\n",
        "valid_data = AddSubDataset(\n",
        "    X_test, y_test, convert_str_to_tokens, SPECIAL_TOKENS, 32, position_encoding_simple\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
        ")\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17ac4856-63fd-4693-87b2-ae616f438202",
      "metadata": {
        "id": "17ac4856-63fd-4693-87b2-ae616f438202"
      },
      "source": [
        "## Part IV: Using transformer on the toy dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fde5c5d",
      "metadata": {
        "id": "0fde5c5d"
      },
      "source": [
        "In this part, we will put all the parts together to train a transformer model. We have implemented most of the functions here for you and your task would be to use these functions to train a Transformer model. The overall tasks are divided into three parts:\n",
        "\n",
        "- Implement the Transformer model using previusly implemented functions\n",
        "- Overfitting the model\n",
        "- Training using complete data\n",
        "- Visualizing the attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "a14ddc39",
      "metadata": {
        "id": "a14ddc39"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0918d6d-0207-406e-9f2e-9a9cc901754f",
      "metadata": {
        "id": "c0918d6d-0207-406e-9f2e-9a9cc901754f"
      },
      "source": [
        "### Implement the Transformer model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f77aa51-af4e-404b-a981-75edd5892623",
      "metadata": {
        "id": "3f77aa51-af4e-404b-a981-75edd5892623"
      },
      "source": [
        "We will add all the peices together to implement the Transformer model completely, as shown in the figure below. Note that till now we have implemented the Encoder and Decoder, and we handled the positional encodings for the input. Whats left is the input and output embedding layer. We will share this embedding layer for the encoder and decoder here. Lastly, we need to map the final output of the decoder to the vocabulary length(the last linear block after decoder in the figure below)\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1snyWKrr2r1J-O8VQTVxkwQYptk0oFhIM\" alt=\"Layer_norm\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7447b1ff",
      "metadata": {
        "id": "7447b1ff"
      },
      "source": [
        "Implement the Transformer model in `transformer.py` and run the cells below to check the final shapes of the output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads: int,\n",
        "        emb_dim: int,\n",
        "        feedforward_dim: int,\n",
        "        num_layers: int,\n",
        "        dropout: float,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        The class encapsulates the implementation of the final Encoder that use\n",
        "        multiple EncoderBlock layers.\n",
        "\n",
        "        args:\n",
        "            num_heads: int representing number of heads to be used in the\n",
        "                EncoderBlock\n",
        "            emb_dim: int repreesenting embedding dimension for the Transformer\n",
        "                model\n",
        "            feedforward_dim: int representing hidden layer dimension for the\n",
        "                feed forward block\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                EncoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, src_seq: Tensor):\n",
        "        for _layer in self.layers:\n",
        "            src_seq = _layer(src_seq)\n",
        "\n",
        "        return src_seq\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads: int,\n",
        "        emb_dim: int,\n",
        "        feedforward_dim: int,\n",
        "        num_layers: int,\n",
        "        dropout: float,\n",
        "        vocab_len: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        The Decoder takes the input from the encoder and the target\n",
        "        sequence to generate the final sequence for the output. We\n",
        "        first pass the input through stacked DecoderBlocks and then\n",
        "        project the output to vocab_len which is required to get the\n",
        "        actual sequence.\n",
        "\n",
        "        args:\n",
        "            num_heads: Int representing number of heads in the MultiheadAttention\n",
        "            for Transformer\n",
        "            emb_dim: int representing the embedding dimension\n",
        "            of the sequence\n",
        "            feedforward_dim: hidden layers in the feed forward block\n",
        "            num_layers: int representing the number of DecoderBlock in Decoder\n",
        "            dropout: float representing the dropout in each DecoderBlock\n",
        "            vocab_len: length of the vocabulary\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                DecoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.proj_to_vocab = nn.Linear(emb_dim, vocab_len)\n",
        "        a = (6 / (emb_dim + vocab_len)) ** 0.5\n",
        "        nn.init.uniform_(self.proj_to_vocab.weight, -a, a)\n",
        "\n",
        "    def forward(self, target_seq: Tensor, enc_out: Tensor, mask: Tensor):\n",
        "\n",
        "        out = target_seq.clone()\n",
        "        for _layer in self.layers:\n",
        "            out = _layer(out, enc_out, mask)\n",
        "        out = self.proj_to_vocab(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "U9KYCgwuJf5o"
      },
      "id": "U9KYCgwuJf5o",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "8d0f19cf",
      "metadata": {
        "id": "8d0f19cf"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads: int,\n",
        "        emb_dim: int,\n",
        "        feedforward_dim: int,\n",
        "        dropout: float,\n",
        "        num_enc_layers: int,\n",
        "        num_dec_layers: int,\n",
        "        vocab_len: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        The class implements Transformer model with encoder and decoder. The input\n",
        "        to the model is a tensor of shape (N, K) and the output is a tensor of shape\n",
        "        (N*O, V). Here, N is the batch size, K is the input sequence length, O is\n",
        "        the output sequence length and V is the Vocabulary size. The input is passed\n",
        "        through shared nn.Embedding layer and then added to input positonal\n",
        "        encodings. Similarily, the target is passed through the same nn.Embedding\n",
        "        layer and added to the target positional encodings. The only difference\n",
        "        is that we take last but one  value in the target. The summed\n",
        "        inputs(look at the code for detials) are then sent through the encoder and\n",
        "        decoder blocks  to get the  final output.\n",
        "        args:\n",
        "            num_heads: int representing number of heads to be used in Encoder\n",
        "                       and decoder\n",
        "            emb_dim: int representing embedding dimension of the Transformer\n",
        "            dim_feedforward: int representing number of hidden layers in the\n",
        "                             Encoder and decoder\n",
        "            dropout: a float representing probability for dropout layer\n",
        "            num_enc_layers: int representing number of encoder blocks\n",
        "            num_dec_layers: int representing number of decoder blocks\n",
        "\n",
        "        \"\"\"\n",
        "        self.emb_layer = None\n",
        "        ##########################################################################\n",
        "        # TODO: Initialize an Embedding layer mapping vocab_len to emb_dim. This #\n",
        "        # is the very first input to our model and transform this input to       #\n",
        "        # emb_dim that will stay the same throughout our model. Please use the   #\n",
        "        # name of this layer as self.emb_layer                                   #\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        self.emb_layer = nn.Embedding(vocab_len, emb_dim)\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "        self.encoder = Encoder(\n",
        "            num_heads, emb_dim, feedforward_dim, num_enc_layers, dropout\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            num_heads,\n",
        "            emb_dim,\n",
        "            feedforward_dim,\n",
        "            num_dec_layers,\n",
        "            dropout,\n",
        "            vocab_len,\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, ques_b: Tensor, ques_pos: Tensor, ans_b: Tensor, ans_pos: Tensor\n",
        "    ) -> Tensor:\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        An implementation of the forward pass of the Transformer.\n",
        "\n",
        "        args:\n",
        "            ques_b: Tensor of shape (N, K) that consists of input sequence of\n",
        "                the arithmetic expression\n",
        "            ques_pos: Tensor of shape (N, K, M) that consists of positional\n",
        "                encodings of the input sequence\n",
        "            ans_b: Tensor of shape (N, K) that consists of target sequence\n",
        "                of arithmetic expression\n",
        "            ans_pos: Tensor of shape (N, K, M) that consists of positonal\n",
        "                encodings of the target sequence\n",
        "\n",
        "        returns:\n",
        "            dec_out: Tensor of shape (N*O, M) where O is the size of\n",
        "                the target sequence.\n",
        "        \"\"\"\n",
        "        q_emb = self.emb_layer(ques_b)\n",
        "        a_emb = self.emb_layer(ans_b)\n",
        "        q_emb_inp = q_emb + ques_pos\n",
        "        a_emb_inp = a_emb[:, :-1] + ans_pos[:, :-1]\n",
        "        dec_out = None\n",
        "        ##########################################################################\n",
        "        # TODO: This portion consists of writing the forward part for the complete\n",
        "        # Transformer. First, pass the q_emb_inp through the encoder, this will be\n",
        "        # the encoder output which you should use as one of the decoder inputs.\n",
        "        # Along with the encoder output, you should also construct an appropriate\n",
        "        # mask using the get_subsequent_mask. Finally, pass the a_emb_inp, the\n",
        "        # encoder output and the mask to the decoder. The task here is to mask\n",
        "        # the values of the target(a_emb_inp)\n",
        "        # Hint: the mask shape will depend on the Tensor ans_b\n",
        "        ##########################################################################\n",
        "        # Replace \"pass\" statement with your code\n",
        "        enc_out=self.encoder(q_emb_inp)\n",
        "        mask=get_subsequent_mask(a_emb_inp)\n",
        "        dec_out=self.decoder(a_emb_inp,enc_out,mask)\n",
        "        dec_out = dec_out.reshape(dec_out.size(0) * dec_out.size(1), dec_out.size(2))\n",
        "        ##########################################################################\n",
        "        #               END OF YOUR CODE                                         #\n",
        "        ##########################################################################\n",
        "\n",
        "        return dec_out\n",
        "\n",
        "inp_seq_len = 9\n",
        "out_seq_len = 5\n",
        "num_heads = 4\n",
        "emb_dim = 32\n",
        "dim_feedforward = 64\n",
        "dropout = 0.2\n",
        "num_enc_layers = 4\n",
        "num_dec_layers = 4\n",
        "vocab_len = len(vocab)\n",
        "\n",
        "model = Transformer(\n",
        "    num_heads,\n",
        "    emb_dim,\n",
        "    dim_feedforward,\n",
        "    dropout,\n",
        "    num_enc_layers,\n",
        "    num_dec_layers,\n",
        "    vocab_len,\n",
        ")\n",
        "for it in train_loader:\n",
        "  it\n",
        "  break\n",
        "inp, inp_pos, out, out_pos = it\n",
        "device = DEVICE\n",
        "model = model.to(device)\n",
        "inp_pos = inp_pos.to(device)\n",
        "out_pos = out_pos.to(device)\n",
        "out = out.to(device)\n",
        "inp = inp.to(device)\n",
        "\n",
        "\n",
        "model_out = model(inp.long(), inp_pos, out.long(), out_pos)\n",
        "assert model_out.size(0) == BATCH_SIZE * (out_seq_len - 1)\n",
        "assert model_out.size(1) == vocab_len"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc890c7f",
      "metadata": {
        "id": "bc890c7f"
      },
      "source": [
        "### Overfitting the model using small data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62a4b8da",
      "metadata": {
        "id": "62a4b8da"
      },
      "source": [
        "Now that we have implemented the Transformer model, lets overfit on a small dataset. This will ensure that the implementation is correct. We keep the training and validation data same here. Before doing that, a couple of things to keep in mind:\n",
        "\n",
        "- We implemented two versions of positional encodings: simple and sinusoid. For overfitting, we will use the simple positional encoding but feel free to experiment with both when training for the complete model\n",
        "- In transformers.py, we have implemented two loss functions for you. The first is the familiar cross entropy loss and second is the `LabelSmoothingLoss`. For overfitting, we will use the cross entropy loss but feel free to experiment with both while doing experiment with the complete data.\n",
        "- Usually, the training regime of Transformers start with a warmup, in other words, we train the model with a lower learning rate for some iterations and then increasing the learning rate to make the network learn faster. Intuitively, this helps you to attain a stable manifold in the loss function and then we increase the learning rate to learn faster in this stable manifold. In a way we are warming up the network to be in a stable manifold and we start training with a higher learning rate after this warm-up. For overfitting we have NOT used this warm-up as for such small data, it is okay to start with a higer learning rate but you should keep this in mind while training with the complete data. We have used two functions from a5_helper.py, `train` and `val`. Here, `train` has three parameters that you should pay attention to:\n",
        "  - `warmup_interval`: Specifies the number of iterations that the network should train with a low learning rate. In other words, its the number of iterations after which the network will have the higher learning rate\n",
        "  - `warmup_lr`: This is the learning rate that will be used during warmup.\n",
        "  - `lr`: This is the learning rate that will get used after the warm-up. If warmup_interval is None, we will start training with this learning rate.\n",
        "\n",
        "In the following cells for overfitting, we have used the number of epochs as 200 but you could increase this. You should get an accuracy ~1 in 200 epochs. It might be a little lower as well, don't worry about it. It should take about a minute to run the overfitting.\n",
        "\n",
        "NOTE: When we say epoch, it means the number of times we have taken a complete pass over the data. One epoch typically consists of many iterations that depend on the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "f9a206d1",
      "metadata": {
        "id": "f9a206d1"
      },
      "outputs": [],
      "source": [
        "def LabelSmoothingLoss(pred, ground):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        pred: predicted tensor of shape (N*O, V) where N is the batch size, O\n",
        "            is the target sequence length and V is the size of the vocab\n",
        "        ground: ground truth tensor of shape (N, O) where N is the batch size, O\n",
        "            is the target sequence\n",
        "    \"\"\"\n",
        "    ground = ground.contiguous().view(-1)\n",
        "    eps = 0.1\n",
        "    n_class = pred.size(1)\n",
        "    one_hot = torch.nn.functional.one_hot(ground).to(pred.dtype)\n",
        "    one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
        "    log_prb = F.log_softmax(pred, dim=1)\n",
        "    loss = -(one_hot * log_prb).sum(dim=1)\n",
        "    loss = loss.sum()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def CrossEntropyLoss(pred, ground):\n",
        "    \"\"\"\n",
        "    args:\n",
        "        pred: predicted tensor of shape (N*O, V) where N is the batch size, O\n",
        "            is the target sequence length and V is the size of the vocab\n",
        "        ground: ground truth tensor of shape (N, O) where N is the batch size, O\n",
        "            is the target sequence\n",
        "    \"\"\"\n",
        "    loss = F.cross_entropy(pred, ground, reduction=\"sum\")\n",
        "    return loss\n",
        "import torch.optim as optim\n",
        "from a5_helper import train as train_transformer\n",
        "from a5_helper import val as val_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "033af962",
      "metadata": {
        "id": "033af962"
      },
      "outputs": [],
      "source": [
        "inp_seq_len = 9\n",
        "out_seq_len = 5\n",
        "num_heads = 4\n",
        "emb_dim = 32\n",
        "dim_feedforward = 32\n",
        "dropout = 0.2\n",
        "num_enc_layers = 1\n",
        "num_dec_layers = 1\n",
        "vocab_len = len(vocab)\n",
        "BATCH_SIZE = 4\n",
        "num_epochs=200 #number of epochs\n",
        "lr=1e-3 #learning rate after warmup\n",
        "loss_func = CrossEntropyLoss\n",
        "warmup_interval = None #number of iterations for warmup\n",
        "\n",
        "model = Transformer(\n",
        "    num_heads,\n",
        "    emb_dim,\n",
        "    dim_feedforward,\n",
        "    dropout,\n",
        "    num_enc_layers,\n",
        "    num_dec_layers,\n",
        "    vocab_len,\n",
        ")\n",
        "train_data = AddSubDataset(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    convert_str_to_tokens,\n",
        "    SPECIAL_TOKENS,\n",
        "    emb_dim,\n",
        "    position_encoding_simple,\n",
        ")\n",
        "valid_data = AddSubDataset(\n",
        "    X_test,\n",
        "    y_test,\n",
        "    convert_str_to_tokens,\n",
        "    SPECIAL_TOKENS,\n",
        "    emb_dim,\n",
        "    position_encoding_simple,\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
        ")\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
        ")\n",
        "\n",
        "small_dataset = torch.utils.data.Subset(\n",
        "    train_data, torch.linspace(0, len(train_data) - 1, steps=4).long()\n",
        ")\n",
        "small_train_loader = torch.utils.data.DataLoader(\n",
        "    small_dataset, batch_size=4, pin_memory=True, num_workers=1, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "8c4fa6c5",
      "metadata": {
        "id": "8c4fa6c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35c5cd0b-e847-42f6-9967-2f8f7a44e660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n",
            "[epoch: 1] [loss:  3.6031 ] val_loss: [val_loss  3.3472 ]\n",
            "[epoch: 2] [loss:  3.4013 ] val_loss: [val_loss  3.1174 ]\n",
            "[epoch: 3] [loss:  2.8290 ] val_loss: [val_loss  2.8851 ]\n",
            "[epoch: 4] [loss:  3.1038 ] val_loss: [val_loss  2.6725 ]\n",
            "[epoch: 5] [loss:  2.9385 ] val_loss: [val_loss  2.4613 ]\n",
            "[epoch: 6] [loss:  2.7472 ] val_loss: [val_loss  2.2790 ]\n",
            "[epoch: 7] [loss:  2.9246 ] val_loss: [val_loss  2.1198 ]\n",
            "[epoch: 8] [loss:  3.0758 ] val_loss: [val_loss  1.9820 ]\n",
            "[epoch: 9] [loss:  2.5098 ] val_loss: [val_loss  1.8603 ]\n",
            "[epoch: 10] [loss:  2.9404 ] val_loss: [val_loss  1.7571 ]\n",
            "[epoch: 11] [loss:  2.1638 ] val_loss: [val_loss  1.6681 ]\n",
            "[epoch: 12] [loss:  1.8994 ] val_loss: [val_loss  1.5943 ]\n",
            "[epoch: 13] [loss:  2.2360 ] val_loss: [val_loss  1.5295 ]\n",
            "[epoch: 14] [loss:  2.2525 ] val_loss: [val_loss  1.4766 ]\n",
            "[epoch: 15] [loss:  1.9887 ] val_loss: [val_loss  1.4292 ]\n",
            "[epoch: 16] [loss:  2.1354 ] val_loss: [val_loss  1.3861 ]\n",
            "[epoch: 17] [loss:  1.9344 ] val_loss: [val_loss  1.3456 ]\n",
            "[epoch: 18] [loss:  1.5167 ] val_loss: [val_loss  1.3123 ]\n",
            "[epoch: 19] [loss:  1.6059 ] val_loss: [val_loss  1.2818 ]\n",
            "[epoch: 20] [loss:  2.1453 ] val_loss: [val_loss  1.2533 ]\n",
            "[epoch: 21] [loss:  1.5768 ] val_loss: [val_loss  1.2277 ]\n",
            "[epoch: 22] [loss:  2.1681 ] val_loss: [val_loss  1.2058 ]\n",
            "[epoch: 23] [loss:  1.4606 ] val_loss: [val_loss  1.1892 ]\n",
            "[epoch: 24] [loss:  1.6068 ] val_loss: [val_loss  1.1705 ]\n",
            "[epoch: 25] [loss:  1.6673 ] val_loss: [val_loss  1.1516 ]\n",
            "[epoch: 26] [loss:  1.7988 ] val_loss: [val_loss  1.1348 ]\n",
            "[epoch: 27] [loss:  1.5694 ] val_loss: [val_loss  1.1180 ]\n",
            "[epoch: 28] [loss:  1.4652 ] val_loss: [val_loss  1.1000 ]\n",
            "[epoch: 29] [loss:  1.4009 ] val_loss: [val_loss  1.0845 ]\n",
            "[epoch: 30] [loss:  1.2520 ] val_loss: [val_loss  1.0709 ]\n",
            "[epoch: 31] [loss:  1.7027 ] val_loss: [val_loss  1.0598 ]\n",
            "[epoch: 32] [loss:  1.7007 ] val_loss: [val_loss  1.0512 ]\n",
            "[epoch: 33] [loss:  1.4435 ] val_loss: [val_loss  1.0428 ]\n",
            "[epoch: 34] [loss:  1.3413 ] val_loss: [val_loss  1.0328 ]\n",
            "[epoch: 35] [loss:  1.6084 ] val_loss: [val_loss  1.0231 ]\n",
            "[epoch: 36] [loss:  1.4928 ] val_loss: [val_loss  1.0119 ]\n",
            "[epoch: 37] [loss:  1.5616 ] val_loss: [val_loss  1.0029 ]\n",
            "[epoch: 38] [loss:  1.1037 ] val_loss: [val_loss  0.9941 ]\n",
            "[epoch: 39] [loss:  1.4261 ] val_loss: [val_loss  0.9843 ]\n",
            "[epoch: 40] [loss:  1.3450 ] val_loss: [val_loss  0.9744 ]\n",
            "[epoch: 41] [loss:  1.1510 ] val_loss: [val_loss  0.9631 ]\n",
            "[epoch: 42] [loss:  1.4262 ] val_loss: [val_loss  0.9530 ]\n",
            "[epoch: 43] [loss:  1.0583 ] val_loss: [val_loss  0.9434 ]\n",
            "[epoch: 44] [loss:  1.3115 ] val_loss: [val_loss  0.9328 ]\n",
            "[epoch: 45] [loss:  1.2397 ] val_loss: [val_loss  0.9239 ]\n",
            "[epoch: 46] [loss:  1.3976 ] val_loss: [val_loss  0.9157 ]\n",
            "[epoch: 47] [loss:  1.2318 ] val_loss: [val_loss  0.9100 ]\n",
            "[epoch: 48] [loss:  1.2987 ] val_loss: [val_loss  0.9036 ]\n",
            "[epoch: 49] [loss:  1.0989 ] val_loss: [val_loss  0.8959 ]\n",
            "[epoch: 50] [loss:  1.0959 ] val_loss: [val_loss  0.8892 ]\n",
            "[epoch: 51] [loss:  1.2879 ] val_loss: [val_loss  0.8815 ]\n",
            "[epoch: 52] [loss:  1.2021 ] val_loss: [val_loss  0.8731 ]\n",
            "[epoch: 53] [loss:  1.1593 ] val_loss: [val_loss  0.8634 ]\n",
            "[epoch: 54] [loss:  1.1348 ] val_loss: [val_loss  0.8529 ]\n",
            "[epoch: 55] [loss:  1.2520 ] val_loss: [val_loss  0.8432 ]\n",
            "[epoch: 56] [loss:  1.1987 ] val_loss: [val_loss  0.8330 ]\n",
            "[epoch: 57] [loss:  1.1680 ] val_loss: [val_loss  0.8248 ]\n",
            "[epoch: 58] [loss:  1.3155 ] val_loss: [val_loss  0.8176 ]\n",
            "[epoch: 59] [loss:  1.0939 ] val_loss: [val_loss  0.8115 ]\n",
            "[epoch: 60] [loss:  1.1318 ] val_loss: [val_loss  0.8071 ]\n",
            "[epoch: 61] [loss:  1.1855 ] val_loss: [val_loss  0.8031 ]\n",
            "[epoch: 62] [loss:  1.1196 ] val_loss: [val_loss  0.7981 ]\n",
            "[epoch: 63] [loss:  1.0106 ] val_loss: [val_loss  0.7927 ]\n",
            "[epoch: 64] [loss:  1.0751 ] val_loss: [val_loss  0.7881 ]\n",
            "[epoch: 65] [loss:  0.8872 ] val_loss: [val_loss  0.7829 ]\n",
            "[epoch: 66] [loss:  1.0244 ] val_loss: [val_loss  0.7783 ]\n",
            "[epoch: 67] [loss:  1.0798 ] val_loss: [val_loss  0.7728 ]\n",
            "[epoch: 68] [loss:  1.0933 ] val_loss: [val_loss  0.7677 ]\n",
            "[epoch: 69] [loss:  1.0054 ] val_loss: [val_loss  0.7634 ]\n",
            "[epoch: 70] [loss:  1.1299 ] val_loss: [val_loss  0.7592 ]\n",
            "[epoch: 71] [loss:  0.9356 ] val_loss: [val_loss  0.7549 ]\n",
            "[epoch: 72] [loss:  1.0813 ] val_loss: [val_loss  0.7516 ]\n",
            "[epoch: 73] [loss:  1.1733 ] val_loss: [val_loss  0.7487 ]\n",
            "[epoch: 74] [loss:  1.1800 ] val_loss: [val_loss  0.7463 ]\n",
            "[epoch: 75] [loss:  1.0390 ] val_loss: [val_loss  0.7444 ]\n",
            "[epoch: 76] [loss:  1.1279 ] val_loss: [val_loss  0.7421 ]\n",
            "[epoch: 77] [loss:  1.1041 ] val_loss: [val_loss  0.7415 ]\n",
            "[epoch: 78] [loss:  0.8905 ] val_loss: [val_loss  0.7415 ]\n",
            "[epoch: 79] [loss:  1.0368 ] val_loss: [val_loss  0.7412 ]\n",
            "[epoch: 80] [loss:  1.0564 ] val_loss: [val_loss  0.7406 ]\n",
            "[epoch: 81] [loss:  1.0439 ] val_loss: [val_loss  0.7402 ]\n",
            "[epoch: 82] [loss:  1.1468 ] val_loss: [val_loss  0.7387 ]\n",
            "[epoch: 83] [loss:  0.8717 ] val_loss: [val_loss  0.7364 ]\n",
            "[epoch: 84] [loss:  0.8524 ] val_loss: [val_loss  0.7330 ]\n",
            "[epoch: 85] [loss:  0.7398 ] val_loss: [val_loss  0.7292 ]\n",
            "[epoch: 86] [loss:  0.9412 ] val_loss: [val_loss  0.7252 ]\n",
            "[epoch: 87] [loss:  0.9249 ] val_loss: [val_loss  0.7195 ]\n",
            "[epoch: 88] [loss:  0.8996 ] val_loss: [val_loss  0.7134 ]\n",
            "[epoch: 89] [loss:  0.8706 ] val_loss: [val_loss  0.7071 ]\n",
            "[epoch: 90] [loss:  0.7654 ] val_loss: [val_loss  0.7005 ]\n",
            "[epoch: 91] [loss:  0.9444 ] val_loss: [val_loss  0.6945 ]\n",
            "[epoch: 92] [loss:  0.9373 ] val_loss: [val_loss  0.6898 ]\n",
            "[epoch: 93] [loss:  0.8449 ] val_loss: [val_loss  0.6859 ]\n",
            "[epoch: 94] [loss:  0.8451 ] val_loss: [val_loss  0.6826 ]\n",
            "[epoch: 95] [loss:  0.8416 ] val_loss: [val_loss  0.6799 ]\n",
            "[epoch: 96] [loss:  0.9184 ] val_loss: [val_loss  0.6768 ]\n",
            "[epoch: 97] [loss:  0.8550 ] val_loss: [val_loss  0.6735 ]\n",
            "[epoch: 98] [loss:  0.8455 ] val_loss: [val_loss  0.6716 ]\n",
            "[epoch: 99] [loss:  0.9037 ] val_loss: [val_loss  0.6698 ]\n",
            "[epoch: 100] [loss:  0.8121 ] val_loss: [val_loss  0.6678 ]\n",
            "[epoch: 101] [loss:  1.0058 ] val_loss: [val_loss  0.6645 ]\n",
            "[epoch: 102] [loss:  0.8639 ] val_loss: [val_loss  0.6602 ]\n",
            "[epoch: 103] [loss:  1.0163 ] val_loss: [val_loss  0.6557 ]\n",
            "[epoch: 104] [loss:  0.7748 ] val_loss: [val_loss  0.6512 ]\n",
            "[epoch: 105] [loss:  0.8234 ] val_loss: [val_loss  0.6476 ]\n",
            "[epoch: 106] [loss:  0.7490 ] val_loss: [val_loss  0.6437 ]\n",
            "[epoch: 107] [loss:  0.7687 ] val_loss: [val_loss  0.6401 ]\n",
            "[epoch: 108] [loss:  1.1026 ] val_loss: [val_loss  0.6356 ]\n",
            "[epoch: 109] [loss:  0.7647 ] val_loss: [val_loss  0.6305 ]\n",
            "[epoch: 110] [loss:  0.7435 ] val_loss: [val_loss  0.6251 ]\n",
            "[epoch: 111] [loss:  0.8204 ] val_loss: [val_loss  0.6198 ]\n",
            "[epoch: 112] [loss:  0.8165 ] val_loss: [val_loss  0.6162 ]\n",
            "[epoch: 113] [loss:  0.8028 ] val_loss: [val_loss  0.6110 ]\n",
            "[epoch: 114] [loss:  1.1045 ] val_loss: [val_loss  0.6055 ]\n",
            "[epoch: 115] [loss:  0.7402 ] val_loss: [val_loss  0.6009 ]\n",
            "[epoch: 116] [loss:  0.7772 ] val_loss: [val_loss  0.5973 ]\n",
            "[epoch: 117] [loss:  0.7457 ] val_loss: [val_loss  0.5950 ]\n",
            "[epoch: 118] [loss:  0.6927 ] val_loss: [val_loss  0.5941 ]\n",
            "[epoch: 119] [loss:  0.7408 ] val_loss: [val_loss  0.5924 ]\n",
            "[epoch: 120] [loss:  0.5857 ] val_loss: [val_loss  0.5910 ]\n",
            "[epoch: 121] [loss:  0.5614 ] val_loss: [val_loss  0.5903 ]\n",
            "[epoch: 122] [loss:  0.8607 ] val_loss: [val_loss  0.5886 ]\n",
            "[epoch: 123] [loss:  0.7239 ] val_loss: [val_loss  0.5850 ]\n",
            "[epoch: 124] [loss:  0.8941 ] val_loss: [val_loss  0.5814 ]\n",
            "[epoch: 125] [loss:  0.7112 ] val_loss: [val_loss  0.5775 ]\n",
            "[epoch: 126] [loss:  0.8126 ] val_loss: [val_loss  0.5734 ]\n",
            "[epoch: 127] [loss:  0.8529 ] val_loss: [val_loss  0.5706 ]\n",
            "[epoch: 128] [loss:  0.7607 ] val_loss: [val_loss  0.5678 ]\n",
            "[epoch: 129] [loss:  0.7894 ] val_loss: [val_loss  0.5652 ]\n",
            "[epoch: 130] [loss:  0.7277 ] val_loss: [val_loss  0.5625 ]\n",
            "[epoch: 131] [loss:  0.8160 ] val_loss: [val_loss  0.5604 ]\n",
            "[epoch: 132] [loss:  0.6992 ] val_loss: [val_loss  0.5585 ]\n",
            "[epoch: 133] [loss:  0.8295 ] val_loss: [val_loss  0.5575 ]\n",
            "[epoch: 134] [loss:  0.6391 ] val_loss: [val_loss  0.5567 ]\n",
            "[epoch: 135] [loss:  0.7569 ] val_loss: [val_loss  0.5557 ]\n",
            "[epoch: 136] [loss:  0.8779 ] val_loss: [val_loss  0.5553 ]\n",
            "[epoch: 137] [loss:  0.7719 ] val_loss: [val_loss  0.5540 ]\n",
            "[epoch: 138] [loss:  0.8167 ] val_loss: [val_loss  0.5530 ]\n",
            "[epoch: 139] [loss:  0.8590 ] val_loss: [val_loss  0.5518 ]\n",
            "[epoch: 140] [loss:  0.6869 ] val_loss: [val_loss  0.5498 ]\n",
            "[epoch: 141] [loss:  0.7858 ] val_loss: [val_loss  0.5474 ]\n",
            "[epoch: 142] [loss:  0.6963 ] val_loss: [val_loss  0.5452 ]\n",
            "[epoch: 143] [loss:  0.8538 ] val_loss: [val_loss  0.5441 ]\n",
            "[epoch: 144] [loss:  0.6416 ] val_loss: [val_loss  0.5435 ]\n",
            "[epoch: 145] [loss:  0.5981 ] val_loss: [val_loss  0.5433 ]\n",
            "[epoch: 146] [loss:  1.0319 ] val_loss: [val_loss  0.5433 ]\n",
            "[epoch: 147] [loss:  0.8105 ] val_loss: [val_loss  0.5436 ]\n",
            "[epoch: 148] [loss:  0.8039 ] val_loss: [val_loss  0.5439 ]\n",
            "[epoch: 149] [loss:  0.6635 ] val_loss: [val_loss  0.5433 ]\n",
            "[epoch: 150] [loss:  0.6741 ] val_loss: [val_loss  0.5415 ]\n",
            "[epoch: 151] [loss:  0.8710 ] val_loss: [val_loss  0.5385 ]\n",
            "[epoch: 152] [loss:  0.6680 ] val_loss: [val_loss  0.5367 ]\n",
            "[epoch: 153] [loss:  0.7692 ] val_loss: [val_loss  0.5364 ]\n",
            "[epoch: 154] [loss:  0.7514 ] val_loss: [val_loss  0.5353 ]\n",
            "[epoch: 155] [loss:  0.6504 ] val_loss: [val_loss  0.5343 ]\n",
            "[epoch: 156] [loss:  0.8456 ] val_loss: [val_loss  0.5317 ]\n",
            "[epoch: 157] [loss:  0.7113 ] val_loss: [val_loss  0.5287 ]\n",
            "[epoch: 158] [loss:  0.7040 ] val_loss: [val_loss  0.5265 ]\n",
            "[epoch: 159] [loss:  0.7737 ] val_loss: [val_loss  0.5244 ]\n",
            "[epoch: 160] [loss:  0.6227 ] val_loss: [val_loss  0.5221 ]\n",
            "[epoch: 161] [loss:  0.8766 ] val_loss: [val_loss  0.5201 ]\n",
            "[epoch: 162] [loss:  0.6978 ] val_loss: [val_loss  0.5184 ]\n",
            "[epoch: 163] [loss:  0.7472 ] val_loss: [val_loss  0.5169 ]\n",
            "[epoch: 164] [loss:  0.8315 ] val_loss: [val_loss  0.5162 ]\n",
            "[epoch: 165] [loss:  0.7794 ] val_loss: [val_loss  0.5164 ]\n",
            "[epoch: 166] [loss:  0.6401 ] val_loss: [val_loss  0.5168 ]\n",
            "[epoch: 167] [loss:  0.8386 ] val_loss: [val_loss  0.5164 ]\n",
            "[epoch: 168] [loss:  0.7752 ] val_loss: [val_loss  0.5163 ]\n",
            "[epoch: 169] [loss:  0.6390 ] val_loss: [val_loss  0.5162 ]\n",
            "[epoch: 170] [loss:  0.6329 ] val_loss: [val_loss  0.5166 ]\n",
            "[epoch: 171] [loss:  0.6846 ] val_loss: [val_loss  0.5156 ]\n",
            "[epoch: 172] [loss:  0.6004 ] val_loss: [val_loss  0.5153 ]\n",
            "[epoch: 173] [loss:  0.6624 ] val_loss: [val_loss  0.5148 ]\n",
            "[epoch: 174] [loss:  0.6497 ] val_loss: [val_loss  0.5136 ]\n",
            "[epoch: 175] [loss:  0.6800 ] val_loss: [val_loss  0.5114 ]\n",
            "[epoch: 176] [loss:  0.6144 ] val_loss: [val_loss  0.5081 ]\n",
            "[epoch: 177] [loss:  0.7009 ] val_loss: [val_loss  0.5057 ]\n",
            "[epoch: 178] [loss:  0.4507 ] val_loss: [val_loss  0.5043 ]\n",
            "[epoch: 179] [loss:  0.5790 ] val_loss: [val_loss  0.5037 ]\n",
            "[epoch: 180] [loss:  0.5975 ] val_loss: [val_loss  0.5026 ]\n",
            "[epoch: 181] [loss:  0.6336 ] val_loss: [val_loss  0.5015 ]\n",
            "[epoch: 182] [loss:  0.5537 ] val_loss: [val_loss  0.5004 ]\n",
            "[epoch: 183] [loss:  0.7354 ] val_loss: [val_loss  0.4994 ]\n",
            "[epoch: 184] [loss:  0.7990 ] val_loss: [val_loss  0.4980 ]\n",
            "[epoch: 185] [loss:  0.5707 ] val_loss: [val_loss  0.4965 ]\n",
            "[epoch: 186] [loss:  0.6779 ] val_loss: [val_loss  0.4952 ]\n",
            "[epoch: 187] [loss:  0.6667 ] val_loss: [val_loss  0.4933 ]\n",
            "[epoch: 188] [loss:  0.7428 ] val_loss: [val_loss  0.4923 ]\n",
            "[epoch: 189] [loss:  0.5414 ] val_loss: [val_loss  0.4912 ]\n",
            "[epoch: 190] [loss:  0.5862 ] val_loss: [val_loss  0.4903 ]\n",
            "[epoch: 191] [loss:  0.6615 ] val_loss: [val_loss  0.4891 ]\n",
            "[epoch: 192] [loss:  0.7084 ] val_loss: [val_loss  0.4886 ]\n",
            "[epoch: 193] [loss:  0.6592 ] val_loss: [val_loss  0.4877 ]\n",
            "[epoch: 194] [loss:  0.6666 ] val_loss: [val_loss  0.4869 ]\n",
            "[epoch: 195] [loss:  0.8739 ] val_loss: [val_loss  0.4865 ]\n",
            "[epoch: 196] [loss:  0.4361 ] val_loss: [val_loss  0.4872 ]\n",
            "[epoch: 197] [loss:  0.6998 ] val_loss: [val_loss  0.4885 ]\n",
            "[epoch: 198] [loss:  0.6194 ] val_loss: [val_loss  0.4898 ]\n",
            "[epoch: 199] [loss:  0.8591 ] val_loss: [val_loss  0.4906 ]\n",
            "[epoch: 200] [loss:  0.6142 ] val_loss: [val_loss  0.4901 ]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Overfitting the model\n",
        "trained_model = train_transformer(\n",
        "    model,\n",
        "    small_train_loader,\n",
        "    small_train_loader,\n",
        "    loss_func,\n",
        "    num_epochs=num_epochs,\n",
        "    lr=lr,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    warmup_interval=warmup_interval,\n",
        "    device=DEVICE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "21a3a5eb",
      "metadata": {
        "id": "21a3a5eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569b7b3b-035b-4ba4-e61c-56d11cfaa2da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overfitted accuracy:  0.6875\n"
          ]
        }
      ],
      "source": [
        "#Overfitted accuracy\n",
        "print(\n",
        "    \"Overfitted accuracy: \",\n",
        "    \"{:.4f}\".format(\n",
        "        val_transformer(\n",
        "            trained_model,\n",
        "            small_train_loader,\n",
        "            CrossEntropyLoss,\n",
        "            batch_size=4,\n",
        "            device=DEVICE,\n",
        "        )[1]\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b43b5e66-7d96-49a7-8d73-649c1d8de2ef",
      "metadata": {
        "id": "b43b5e66-7d96-49a7-8d73-649c1d8de2ef"
      },
      "source": [
        "### Fitting the model using complete data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f3f4cfc-d267-436e-ae7a-36e3ab40e7c1",
      "metadata": {
        "id": "3f3f4cfc-d267-436e-ae7a-36e3ab40e7c1"
      },
      "source": [
        "Run the below cells to fit the model using the complete data. Keep in mind the various things you could experiment with here, losses, positional encodings, warm up routines and learning rates. You could also play with the size of the model but that will require more time to train on Colab.\n",
        "\n",
        "You should aim for final validation accuracy of ~80 percent.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "352896e6",
      "metadata": {
        "id": "352896e6"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "inp_seq_len = 9\n",
        "out_seq_len = 5\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "#You should change these!\n",
        "\n",
        "num_heads = 4\n",
        "emb_dim = 32\n",
        "dim_feedforward = 32\n",
        "dropout = 0.2\n",
        "num_enc_layers = 4\n",
        "num_dec_layers = 4\n",
        "vocab_len = len(vocab)\n",
        "loss_func = CrossEntropyLoss\n",
        "poss_enc = position_encoding_simple\n",
        "num_epochs = 10\n",
        "warmup_interval = None\n",
        "lr = 1e-3\n",
        "\n",
        "\n",
        "model = Transformer(\n",
        "    num_heads,\n",
        "    emb_dim,\n",
        "    dim_feedforward,\n",
        "    dropout,\n",
        "    num_enc_layers,\n",
        "    num_dec_layers,\n",
        "    vocab_len,\n",
        ")\n",
        "\n",
        "\n",
        "train_data = AddSubDataset(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    convert_str_to_tokens,\n",
        "    SPECIAL_TOKENS,\n",
        "    emb_dim,\n",
        "    position_encoding_sinusoid,\n",
        ")\n",
        "valid_data = AddSubDataset(\n",
        "    X_test,\n",
        "    y_test,\n",
        "    convert_str_to_tokens,\n",
        "    SPECIAL_TOKENS,\n",
        "    emb_dim,\n",
        "    position_encoding_sinusoid,\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
        ")\n",
        "valid_loader = torch.utils.data.DataLoader(\n",
        "    valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "43bfb054",
      "metadata": {
        "id": "43bfb054",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518210e2-299a-4ff9-9187-b5963813e605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training started...\n",
            "[epoch: 1] [loss:  3.0120 ] val_loss: [val_loss  2.4945 ]\n",
            "[epoch: 2] [loss:  2.6923 ] val_loss: [val_loss  2.3884 ]\n",
            "[epoch: 3] [loss:  2.5759 ] val_loss: [val_loss  2.3647 ]\n",
            "[epoch: 4] [loss:  2.5198 ] val_loss: [val_loss  2.3532 ]\n",
            "[epoch: 5] [loss:  2.4845 ] val_loss: [val_loss  2.3520 ]\n",
            "[epoch: 6] [loss:  2.4735 ] val_loss: [val_loss  2.3466 ]\n",
            "[epoch: 7] [loss:  2.4614 ] val_loss: [val_loss  2.3452 ]\n",
            "[epoch: 8] [loss:  2.4377 ] val_loss: [val_loss  2.3442 ]\n",
            "[epoch: 9] [loss:  2.4332 ] val_loss: [val_loss  2.3447 ]\n",
            "[epoch: 10] [loss:  2.4208 ] val_loss: [val_loss  2.3431 ]\n"
          ]
        }
      ],
      "source": [
        "#Training the model with complete data\n",
        "trained_model = train_transformer(\n",
        "    model,\n",
        "    train_loader,\n",
        "    valid_loader,\n",
        "    loss_func,\n",
        "    num_epochs,\n",
        "    lr = lr,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    warmup_interval=warmup_interval,\n",
        "    device=DEVICE\n",
        ")\n",
        "weights_path = os.path.join(GOOGLE_DRIVE_PATH, \"transformer.pt\")\n",
        "torch.save(trained_model.state_dict(), weights_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "345b9bae",
      "metadata": {
        "id": "345b9bae"
      },
      "source": [
        "Run the cell below to get the accuracy on the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "0ad70d5b",
      "metadata": {
        "id": "0ad70d5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc376ec-d4f5-421a-fc6e-59fcbd57c08d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model accuracy:  0.2500\n"
          ]
        }
      ],
      "source": [
        "#Final validation accuracy\n",
        "print(\n",
        "    \"Final Model accuracy: \",\n",
        "    \"{:.4f}\".format(\n",
        "        val_transformer(\n",
        "            trained_model, valid_loader, LabelSmoothingLoss, 4, device=DEVICE\n",
        "        )[1]\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c19615d0-9b82-42ac-ba28-e079ca96aed2",
      "metadata": {
        "id": "c19615d0-9b82-42ac-ba28-e079ca96aed2"
      },
      "source": [
        "## Visualize and Inference: Model in action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f413c17-ede5-4d4d-a1fa-54458ca948c5",
      "metadata": {
        "id": "3f413c17-ede5-4d4d-a1fa-54458ca948c5"
      },
      "source": [
        "Now that we have trained a model, lets look at the final results. We will first look at the results from the validation data and visualize the attention weights (remember the self.weights_softmax?). These attention weights should give you some intuition about what the network learns. We have implemented everything for you here and the intention is to help you probe the model and understand about what does the network learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "c39117ef-da8b-4fb0-904b-c5c4c37fc800",
      "metadata": {
        "id": "c39117ef-da8b-4fb0-904b-c5c4c37fc800"
      },
      "outputs": [],
      "source": [
        "import seaborn\n",
        "from a5_helper import inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "137296b8-8ab8-4f9d-bff5-e2584370a757",
      "metadata": {
        "id": "137296b8-8ab8-4f9d-bff5-e2584370a757"
      },
      "source": [
        "### Results from the validation data\n",
        "\n",
        "In the below cell we pick the very first data point in the validation data and find the result on it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "37c01cc4-96b0-4a4e-a820-64e5c2dd5549",
      "metadata": {
        "id": "37c01cc4-96b0-4a4e-a820-64e5c2dd5549",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f3b5d24-8971-48b3-b35e-4a91a8a52fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence: \n",
            " BOS POSITIVE 47 add NEGATIVE 27 EOS\n"
          ]
        }
      ],
      "source": [
        "for it in valid_loader:\n",
        "    it\n",
        "    break\n",
        "inp, inp_pos, out, out_pos = it\n",
        "opposite_tokens_to_str = {v: k for k, v in convert_str_to_tokens.items()}\n",
        "device = torch.device(\"cuda\")\n",
        "model = model.to(device)\n",
        "inp_pos = inp_pos.to(device)\n",
        "out_pos = out_pos.to(device)\n",
        "out = out.to(device)\n",
        "inp = inp.to(device)\n",
        "\n",
        "inp_exp = inp[:1, :]\n",
        "inp_exp_pos = inp_pos[:1]\n",
        "out_pos_exp = out_pos[:1, :]\n",
        "inp_seq = [opposite_tokens_to_str[w.item()] for w in inp_exp[0]]\n",
        "print(\n",
        "    \"Input sequence: \\n\",\n",
        "    inp_seq[0]\n",
        "    + \" \"\n",
        "    + inp_seq[1]\n",
        "    + \" \"\n",
        "    + inp_seq[2]\n",
        "    + inp_seq[3]\n",
        "    + \" \"\n",
        "    + inp_seq[4]\n",
        "    + \" \"\n",
        "    + inp_seq[5]\n",
        "    + \" \"\n",
        "    + inp_seq[6]\n",
        "    + inp_seq[7]\n",
        "    + \" \"\n",
        "    + inp_seq[8],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "ab535f1d-3be0-40ef-a0fd-8fca2fae2414",
      "metadata": {
        "id": "ab535f1d-3be0-40ef-a0fd-8fca2fae2414",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b637b9e4-c6b5-469c-e09b-4cf62d8f0498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Sequence:\tBOS \n"
          ]
        }
      ],
      "source": [
        "out_seq_ans, _ = inference(\n",
        "    trained_model, inp_exp, inp_exp_pos, out_pos_exp, out_seq_len\n",
        ")\n",
        "\n",
        "trained_model.eval()\n",
        "\n",
        "print(\"Output Sequence:\", end=\"\\t\")\n",
        "res = \"BOS \"\n",
        "for i in range(1, out_seq_ans.size(1)):\n",
        "    sym = opposite_tokens_to_str[out_seq_ans[0, i].item()]\n",
        "    if sym == \"EOS\":\n",
        "        break\n",
        "    res += sym + \" \"\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3412c073-d239-450a-aa46-9ec3d61309a6",
      "metadata": {
        "id": "3412c073-d239-450a-aa46-9ec3d61309a6"
      },
      "source": [
        "### Pick your own proboing example\n",
        "\n",
        "In the cell below, you could feed in an example in the input style, changing the variable `custom_seq`. We have filled a placeholder expression for you, but feel free to change it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "d7d2f3b1-ba94-4ccf-84e3-f3238059f25d",
      "metadata": {
        "id": "d7d2f3b1-ba94-4ccf-84e3-f3238059f25d"
      },
      "outputs": [],
      "source": [
        "custom_seq = \"BOS POSITIVE 02 subtract NEGATIVE 07 EOS\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "3caa768d-f61f-4ee4-8e87-259043e93cdb",
      "metadata": {
        "id": "3caa768d-f61f-4ee4-8e87-259043e93cdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7646d934-bf75-4b53-d88a-e6ac3b75b6c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Sequence:\tBOS \n"
          ]
        }
      ],
      "source": [
        "out = prepocess_input_sequence(custom_seq, convert_str_to_tokens, SPECIAL_TOKENS)\n",
        "inp_exp = torch.tensor(out).to(DEVICE)\n",
        "\n",
        "out_seq_ans, model_for_visv = inference(\n",
        "    trained_model, inp_exp, inp_exp_pos, out_pos_exp, out_seq_len\n",
        ")\n",
        "\n",
        "trained_model.eval()\n",
        "\n",
        "print(\"Output Sequence:\", end=\"\\t\")\n",
        "res = \"BOS \"\n",
        "for i in range(1, out_seq_ans.size(1)):\n",
        "    sym = opposite_tokens_to_str[out_seq_ans[0, i].item()]\n",
        "    if sym == \"EOS\":\n",
        "        break\n",
        "    res += sym + \" \"\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b9ae8cd-2813-4845-a4df-d47ebdc60971",
      "metadata": {
        "id": "0b9ae8cd-2813-4845-a4df-d47ebdc60971"
      },
      "source": [
        "### Visualize the attention weights\n",
        "\n",
        "In this part we will visualize the attention weights for the specific custom input you fed as input. There are seperate heatmaps for encoder and the decoder. The ligher value in color shows higher associated between the token present in that row and column, and darker color shows a weak relation between them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "8e82ff56-d55b-45b7-8507-514ee5968ed5",
      "metadata": {
        "id": "8e82ff56-d55b-45b7-8507-514ee5968ed5"
      },
      "outputs": [],
      "source": [
        "from a5_helper import draw\n",
        "import seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "db22789f",
      "metadata": {
        "id": "db22789f"
      },
      "outputs": [],
      "source": [
        "target_exp = res.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "9cae7e26-2587-4297-a44c-c59ab541dfc8",
      "metadata": {
        "id": "9cae7e26-2587-4297-a44c-c59ab541dfc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60e91506-328a-4b69-ab4f-599096fc7e51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Block Number 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'EncoderBlock' object has no attribute 'MultiHeadBlock'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-55368d16e029>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         draw(\n\u001b[1;32m      6\u001b[0m             \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mMultiHeadBlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mweights_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'EncoderBlock' object has no attribute 'MultiHeadBlock'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for layer in range(num_enc_layers):\n",
        "    fig, axs = plt.subplots(1, num_heads, figsize=(20, 10))\n",
        "    print(\"Encoder Block Number\", layer + 1)\n",
        "    for h in range(num_heads):\n",
        "        draw(\n",
        "            trained_model.encoder.layers[layer]\n",
        "            .MultiHeadBlock.heads[h]\n",
        "            .weights_softmax.data.cpu()\n",
        "            .numpy()[0],\n",
        "            inp_seq,\n",
        "            inp_seq if h == 0 else [],\n",
        "            ax=axs[h],\n",
        "        )\n",
        "    plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "be6ba6cb",
      "metadata": {
        "id": "be6ba6cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d6c1b8d0-5a98-4354-b34a-8d4af9f83446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Block number  1\n",
            "Decoder Self Attention 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Cross attention 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Block number  2\n",
            "Decoder Self Attention 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Cross attention 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Block number  3\n",
            "Decoder Self Attention 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Cross attention 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABjkAAAD4CAYAAAC+GYTXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAArkElEQVR4nO3de7DVdb34/9dagIAom4vyRRMJanNpgDxpWXkA7WimhUqS5dEEBi8UY5eTx6xpxksntcasaZxJCTUto0IxvJRloKVsDcUUD3ghL0gnMQkE0Y0o+/37o5+7tmxib/2s9fl89n48Zphxrc8a9kvWZ6/n1hdrfSoppRQAAAAAAAAlU817AAAAAAAAgDfDkgMAAAAAACglSw4AAAAAAKCULDkAAAAAAIBSsuQAAAAAAABKyZIDAAAAAAAoJUsOAAAAAACglCw5AAAAAACAUrLkAAAAAAAASqln3gO8buRe/5b3CDvYvUfvvEdoV69Kj7xHKIXnt23Ke4R2FfH5G7xb/7xHaNeTW9blPcIOJg4cnfcI7Vr0zC15j1AY/69hTN4jtOszAw7Me4QdXPzcXXmPsIPdehTmR5M29tytb94j7OC0/gfkPUK7Lnm+Ke8RdpAi5T1Cu97Rf5+8R9jBinX35D1CYbxjr/fkPUK7GnrtnvcIO/hL89/yHqFd/Xv1y3uEHbza8mreI7TrwD2G5z3CDpZu+lPeI+xgSJ8BeY/Qrh6V4v391QeevTvvEQpl6v5T8h5hBw++9EzeI7SrX8/i/dy99qXn8x6hXQ27Fe9ngqF9BuU9wg5ebXkt7xHatWX71rxH2MFu1WL+9/jK5/6wy8cUr4QAAAAAAAAdYMkBAAAAAACUkiUHAAAAAABQSpYcAAAAAABAKVlyAAAAAAAApWTJAQAAAAAAlFImS441a9bEqlWroqWlJYvfDgAAAAAAYJc6teS46qqr4tJLL21z3+mnnx4jR46M8ePHx7hx42Lt2rWZDggAAAAAANCeTi055s6dGwMHDmy9fdttt8XVV18d1157bdx3330xYMCAOP/88zMfEgAAAAAA4I16dubBq1evjoMOOqj19qJFi+LYY4+Nk046KSIiLrzwwpg5c2a2EwIAAAAAALSjU+/kaG5ujv79+7febmpqikmTJrXeHjlyZKxbty676QAAAAAAAHaiU0uO4cOHx/LlyyMiYv369bFy5co45JBDWo+vW7cuGhoasp0QAAAAAACgHZ36uKrp06fHnDlzYuXKlbFkyZIYM2ZMHHjgga3Hm5qaYty4cZkPCQAAAAAA8EadWnKcffbZ8fLLL8fChQtj6NChsWDBgjbHly5dGieeeGKmAwIAAAAAALSnU0uOarUaF1xwQVxwwQXtHn/j0gMAAAAAAKBWOrXkeF1zc3Pcfvvt8fjjj0dExKhRo+KII46Ivn37ZjocAAAAAADAznR6yXHTTTfFqaeeGuvXr29z/1577RVXXnllTJkyZZe/xyuvvBKvvPJKm/tSaolKpVPXQQcAAAAAALqxTm0VmpqaYtq0aTFp0qRYunRpbNiwITZs2BB33313TJw4MaZNmxb33nvvLn+fiy66KBoaGtr8eqH5uTf9LwEAAAAAAHQ/nVpy/M///E/MnDkzrr/++vjABz4QAwYMiAEDBsQHP/jBuOGGG2LGjBk7vV7HP/vKV74SmzZtavNrQN//96b/JQAAAAAAgO6nUx9Xde+998Y3v/nNnR6fM2dOTJ48eZe/T+/evaN3795t7vNRVQAAAAAAQGd0arPQ3Nwc/fv33+nxhoaG2Lp161seCgAAAAAAYFc6teRobGyMJUuW7PT44sWLo7Gx8S0PBQAAAAAAsCudWnLMnDkzzjrrrPjlL3+5w7Fbb701zj777JgxY0ZWswEAAAAAAOxUp67J8fnPfz6ampriYx/7WIwePTrGjh0bKaV45JFH4vHHH4+pU6fGF77whRqNCgAAAAAA8A+deidHtVqNBQsWxPz582PUqFHx6KOPxmOPPRZjxoyJn/zkJ3HDDTdEteoC4gAAAAAAQO29qY3E4YcfHosWLYpVq1bFbbfdFhMmTIjly5fHXXfdlfV8AAAAAAAA7erUkuPhhx+Ot7/97TFkyJAYM2ZMPPjgg/G+970vvvOd78TcuXPjsMMOi1/84hc1GhUAAAAAAOAfOrXkOPvss2P8+PHx+9//Pg499ND42Mc+FkcffXRs2rQpNm7cGGeccUZcfPHFtZoVAAAAAACgVacuPH7ffffFkiVLYsKECfHud7875s6dG3PmzGm9DseZZ54Z73//+2syKAAAAAAAwD/r1Ds5NmzYEEOHDo2IiD322CP69esXAwcObD0+cODAePHFF7OdEAAAAAAAoB2dvvB4pVL5l7cBAAAAAADqoVMfVxURMWPGjOjdu3dERGzdujVmz54d/fr1i4iIV155JdvpAAAAAAAAdqJTS47p06e3uX3yySfv8JhTTjnlrU0EAAAAAADQAZ1aclx99dW1mgMAAAAAAKBTOn1NDgAAAAAAgCKw5AAAAAAAAEqp0xce706qlUreI5TG9mjJe4QdpJTyHqF9TqsOK+L3YI+K3XDRVQp43tBxRX3+ijjXq5WCdq6AKgWNbw9/36jQivhzSESBf8alQyoF/Vlyt+iR9wilsK3ltbxHaNduVf9rh85riWL2pIj9rRb0Z8kiSgU9r4qoiP+N0lLinzOL+RMWAAAAAADALlhyAAAAAAAApWTJAQAAAAAAlJIlBwAAAAAAUEqWHAAAAAAAQClZcgAAAAAAAKVkyQEAAAAAAJSSJQcAAAAAAFBKlhwAAAAAAEApWXIAAAAAAAClZMkBAAAAAACUkiUHAAAAAABQSpYcAAAAAABAKVlyAAAAAAAApWTJAQAAAAAAlJIlBwAAAAAAUEqWHAAAAAAAQClZcgAAAAAAAKVkyQEAAAAAAJSSJQcAAAAAAFBKlhwAAAAAAEApWXIAAAAAAAClZMkBAAAAAACUkiUHAAAAAABQSpYcAAAAAABAKVlyAAAAAAAApWTJAQAAAAAAlJIlBwAAAAAAUEqWHAAAAAAAQClZcgAAAAAAAKVkyQEAAAAAAJSSJQcAAAAAAFBKlhwAAAAAAEApWXIAAAAAAAClZMkBAAAAAACUkiUHAAAAAABQSj3zHgCgTFJKeY8AAAAAAPz/vJMDAAAAAAAoJUsOAAAAAACglCw5AAAAAACAUrLkAAAAAAAASsmSAwAAAAAAKCVLDgAAAAAAoJQsOQAAAAAAgFKy5AAAAAAAAErJkgMAAAAAACglSw4AAAAAAKCULDkAAAAAAIBSsuQAAAAAAABKyZIDAAAAAAAoJUsOAAAAAACglCw5AAAAAACAUrLkAAAAAAAASsmSAwAAAAAAKCVLDgAAAAAAoJQsOQAAAAAAgFKy5AAAAAAAAErJkgMAAAAAACglSw4AAAAAAKCULDkAAAAAAIBSsuQAAAAAAABKyZIDAAAAAAAoJUsOAAAAAACglCw5AAAAAACAUrLkAAAAAAAASsmSAwAAAAAAKCVLDgAAAAAAoJQsOQAAAAAAgFKy5AAAAAAAAErJkgMAAAAAACglSw4AAAAAAKCULDkAAAAAAIBSsuQAAAAAAABKyZIDAAAAAAAopZ55D1BkLSnlPUK7elTyngAAAICuIkUx/9uXjqmG/0kAQPfmnRwAAAAAAEApWXIAAAAAAAClZMkBAAAAAACUkiUHAAAAAABQSpYcAAAAAABAKVlyAAAAAAAApWTJAQAAAAAAlJIlBwAAAAAAUEqWHAAAAAAAQClZcgAAAAAAAKVkyQEAAAAAAJSSJQcAAAAAAFBKlhwAAAAAAEApWXIAAAAAAAClZMkBAAAAAACUkiUHAAAAAABQSpYcAAAAAABAKVlyAAAAAAAApWTJAQAAAAAAlJIlBwAAAAAAUEqWHAAAAAAAQClZcgAAAAAAAKVkyQEAAAAAAJSSJQcAAAAAAFBKlhwAAAAAAEApWXIAAAAAAAClZMkBAAAAAACUkiUHAAAAAABQSpYcAAAAAABAKVlyAAAAAAAApWTJAQAAAAAAlJIlBwAAAAAAUEqWHAAAAAAAQClZcgAAAAAAAKVkyQEAAAAAAJSSJQcAAAAAAFBOqQvZunVrOvfcc9PWrVvzHqWNIs5VxJlSKuZcRZwppWLOVcSZUirmXEWcibaK+BwVcaaUijlXEWdKqZhzmanjijhXEWeiraI+R0Wcy0wdV8S5ijhTSsWcq4gzpVTcufi7oj4/RZyriDOlVMy5ijhTSsWcq4gzpVTMufKYqZJSSnkvWrKyefPmaGhoiE2bNkX//v3zHqdVEecq4kwRxZyriDNFFHOuIs4UUcy5ijgTbRXxOSriTBHFnKuIM0UUcy4zdVwR5yriTLRV1OeoiHOZqeOKOFcRZ4oo5lxFnCmiuHPxd0V9foo4VxFniijmXEWcKaKYcxVxpohizpXHTD6uCgAAAAAAKCVLDgAAAAAAoJQsOQAAAAAAgFLqUkuO3r17x7nnnhu9e/fOe5Q2ijhXEWeKKOZcRZwpophzFXGmiGLOVcSZaKuIz1ERZ4oo5lxFnCmimHOZqeOKOFcRZ6Ktoj5HRZzLTB1XxLmKOFNEMecq4kwRxZ2Lvyvq81PEuYo4U0Qx5yriTBHFnKuIM0UUc648ZupSFx4HAAAAAAC6jy71Tg4AAAAAAKD7sOQAAAAAAABKyZIDAAAAAAAoJUsOAAAAAACglCw5ujHXnAcgC3oCQFY0BYAs6Al0L5Yc3Vjv3r3jkUceyXsMAEpOTwDIiqYAkAU9ge6lZ94D1MKaNWvipZdeijFjxkS1ao/zX//1X+3ev3379rj44otj8ODBERFx6aWX1nOsOPPMM+OEE06IiRMn1vXr7szmzZs7/Nj+/fvXcJLOSynF888/H0OGDKnr121ubo7ly5fHoEGD4l3velebY1u3bo2f//znccopp9R1pqKdV5SbnrSlJx2jJ52nJ3QHmtKWpnSMpnRe0ZpStHOK8tOTtvSkY/Sk84rWk4jinVeFkkrsyiuvTN/+9rfb3HfaaaelarWaqtVqGjt2bHrmmWfqOtNRRx2VXnjhhdbbF110Udq4cWPr7fXr16exY8fWdaZKpZIOOOCAdOihh7b5ValU0nvf+9506KGHpsMOO6yuM70+V7VaTY2Njeniiy9Ozz77bN1naG+ejvyqt759+6a//vWvrbePPvro9Je//KX19rp16+o+12OPPZaGDx/e+uc2adKk3GdKqXjnVUopvfrqq2nr1q1t7lu3bl0677zz0n//93+nu+66K6fJeF0Re5JS8ZqiJ52bR086Rk86R1OKr4hNKVpPUtKUzs6jKR1TxKYU7Zx6nZ4Un550jJ50bh496Zgi9iSl4p1XKRWnJ6Vechx88MHpqquuar39q1/9KvXs2TP9+Mc/TsuXL08f+MAH0qxZs+o6U7VaTc8991zr7T333DM98cQTrbfz+Ca46KKL0ogRI9LixYvb3N+zZ8+0cuXKus7yzyqVSvrtb3+bPv/5z6e99tor9erVKx1zzDHp5ptvTtu3b6/7PHfeeWfrrx/+8Idp6NCh6ZxzzkmLFi1KixYtSuecc07aZ5990g9/+MO6z1apVNqcV3vssccO51WlUqnrTMcdd1z66Ec/mp5//vm0evXq9NGPfjSNGDEirVmzpnWmvF7wi3RepZTSjBkz0umnn956e/PmzWnYsGFp7733ThMmTEg9e/ZMt956ay6z8XdF7ElKxWuKnnSMnnSOnnSOphRfEZtStJ6kpCkdpSmdU8SmFO2cep2eFJ+edIyedIyedE4Re5JS8c6rlIrTk1IvOQYNGpRWrFjRenv27Nnp+OOPb719xx13pLe//e11nakj35h5fBMsW7YsjRo1Kn3pS19K27ZtSykV4wX/9T+rbdu2pZ/97GfpyCOPTD169Ej77rtv+upXv5pWr16dy2wf+tCH0k9+8pMd7r/uuuvS5MmT6z5PEc+rIUOGtPn+a2lpSbNnz077779/euKJJ3J9wS/aedXY2Jh+/etft96+7LLL0r777tv6N2DOPvvsdOihh9Z1JtoqYk9SKub3vp50jp7smp50jqYUXxGbUsTv/ZQ0pbM0ZdeK2JSinlN6Unx60nF60jl6smtF7ElKxTyvitKTUn94X3Nzc5vPiWtqaopJkya13h45cmSsW7cuj9EK573vfW8sX748nn/++TjooIPif//3f6NSqeQ9VqtevXrFCSecELfddls8+eSTcdppp8V1110Xo0ePzmWee+65Jw466KAd7j/ooINi2bJlOUxUPM3NzdGz5z8u61OpVOL73/9+TJkyJSZPnhyPP/54jtP9XVHOq//7v/+LxsbG1tuLFy+O448/PhoaGiIiYvr06bFy5cq6zkRbetJxetI5erJretI5mlJ8mtJxmtI5mrJrRW9Kkc4pPSk+Pek4PekcPdm1ovckojjnVVF6Uuolx/Dhw2P58uUREbF+/fpYuXJlHHLIIa3H161b1/oHWi+VSmWHF9KivLDusccecc0118RXvvKVOPzww2P79u15j9Su/fffP84777x46qmn4rbbbstlhmHDhsUPfvCDHe6fN29eDBs2rO7zvPG8au88q7cxY8bE/fffv8P9l112WRx77LFxzDHH5DDVzuV5XvXp0yeam5tbb997771x8MEHtzm+ZcuWus5EW0XsSURxm6InHacnu6YnnaMpxVfEphS1JxGa0hmasmtlakre55SeFJ+edI6edJye7FqZehLh/3lFRPTc9UOKa/r06TFnzpxYuXJlLFmyJMaMGRMHHnhg6/GmpqYYN25cXWdKKcWMGTOid+/eERGxdevWmD17dvTr1y8iIl555ZW6ztOeT33qU/Hv//7vsXz58hg+fHhucwwfPjx69Oix0+OVSiWOOOKIOk70D9/5znfi+OOPj1/96let35jLli2L1atXxw033FD3eVJKMWrUqNYX+S1btsS//du/RbVabT1eb1OnTo358+fHpz/96R2OXXbZZdHS0hKXX3553ecq4nl1wAEHxI9+9KO46KKL4q677ornnnsuPvShD7Uef+KJJ2Lfffet60y0VcSeRBS/KXqya3qya3rSOZpSfEVsStF7EqEpHaEpu1bEphT1nNKT4tOTN0dPdk1Pdq2IPYko5nlVlJ5UUh5nSkZaWlrivPPOi5tvvjmGDh0al156aYwdO7b1+Cc+8Yn4yEc+ErNmzarbTDNmzOjQtvHqq6+uwzS8FWvXro3vf//78eijj0ZExNixY2P27Nm5bLWvueaaDj1u+vTpNZ6EN+N3v/tdHHXUUbHPPvvEs88+GyeeeGJceeWVrcc/+9nPxksvvdTh55nsFbEnEZrSVegJWdKU4itiU/Sk69AUsqInxacn1JKekJWi9KTUSw7oLrZv3/4vN7UU3yOPPBK/+c1vYujQofGJT3yi9W8kRETMnTs33ve+98UBBxyQ34BAt6AnXYOmAEWgKeWnJ0AR6En5FaEnXWLJ0dzcHLfffnvrRV9GjRoVRxxxRPTt27fus0ybNi1OPfXUOPLII3P//Dg6Z8WKFR1+7IQJE2o4yY722WefmD59esyaNavNxXyAbBWpJxGaUlZ6AkQUqyl6Ul6aAugJWdATurrSLzluuummOPXUU2P9+vVt7t9rr73iyiuvjClTptR1nv/4j/+IO++8M/bdd9+YOXNmzJgxI0aOHFnXGXhzqtVqVCqVSCm1ifXr3yL/fF+9L2D19a9/Pa655pp46qmn4oMf/GDMmjUrTjjhhNh9993rOgdvzYIFC2L+/Pltfjj9z//8z5g2bVrOkxFRvJ5EaEpZ6Qn1oCnFVrSm6El5aQq1pifFpidkRU+otdx7kkps6dKlqVevXun4449PTU1NaePGjWnjxo1p6dKl6eMf/3jabbfd0j333FP3uZ5++ul07rnnphEjRqRqtZoOO+ywdN1116WtW7fWfRY67umnn279deONN6Z3vOMd6fLLL08PPfRQeuihh9Lll1+eGhsb04033pjbjHfccUc65ZRTUr9+/VL//v3Tqaeemu69997c5qFjtm/fnk444YRUqVTS6NGj07HHHpuOPfbYNGrUqFStVtMnP/nJ1NLSkveY3VpRe5KSppSRnlBLmlJ8RW2KnpSTplArelJ8ekKW9IRaKUpPSr3kOOqoo9Lpp5++0+Onn356Ouqoo+o40Y4WL16cTjrppLT77rungQMHps9+9rPp/vvvz3Umdu29731vuvXWW3e4/9Zbb03vec97cpiorRdffDH94Ac/SIccckiqVCrpXe96V/r2t7+d91jsxKWXXpoGDRqUbr755h2OLVq0KA0aNCh95zvfqf9gtCpDT1LSlDLSE7KmKcVXhqboSTlpClnSk+LTE2pFT8hSUXpS6iXHwIED04oVK3Z6/KGHHkoDBgyo40Q7t3nz5nT55ZenQYMGpR49euQ9DrvQp0+ftGrVqh3uX7VqVerTp08OE+3cLbfckgYNGpSq1Wreo7AT48ePT1deeeVOj8+bNy+NHz++jhPxRmXqSUqaUiZ6QtY0pfjK1BQ9KRdNIUt6Unx6Qq3oCVkqSk+qu/5Aq+Jqbm6O/v377/R4Q0NDbN26tY4Tte+pp56KSy65JC688MLYtGlTHH744XmPxC6MHTs2Lrrooti2bVvrfdu2bYuLLrooxo4dm+Nkf/fyyy/HD3/4w5g8eXIcc8wxMXjw4PjGN76R91jsxOrVq//l9/3hhx8eq1evruNEvFFZehKhKWWjJ2RNU4qvLE3Rk/LRFLKkJ8WnJ9SKnpClovSkZ82/Qg01NjbGkiVLYubMme0eX7x4cTQ2NtZ5qr/bunVrXH/99XHVVVfF73//+xg2bFjMmjUrZs6cGcOGDctlJjru8ssvjylTpsR+++0XEyZMiIiIFStWRETELbfckttcTU1NcdVVV8WCBQvitddei2nTpsXXv/71mDRpUm4zsWt9+/aNF154Ifbff/92j2/evDn69OlT56n4Z0XuSYSmlJmekDVNKb4iN0VPyk1TyJKeFJ+eUCt6QpYK05Oav1ekhl7/zK/2PkfulltuSYMHD677Z7b94Q9/SGeccUYaMGBA6tOnTzrxxBPT7bff7oJdJbRly5Z0xRVXpC9+8Yvpi1/8Ypo7d27asmVLLrN885vfTGPGjEnVajW9733vS1dccUXavHlzLrPQeUcffXSaPXv2To+fccYZuX+WandXxJ6kpCldhZ6QJU0pviI2RU+6Dk0hK3pSfHpCLekJWSlKTyoppVT7VUpttLS0xCc/+cm44YYbYvTo0TF27NhIKcUjjzwSjz/+eEydOjUWLFgQ1Wr9PpWrWq3Gu9/97pg1a1acdNJJMXDgwLp9bbK3atWqeOaZZ9q8hS8i4phjjqnrHHvvvXecfPLJMWvWrBg3blxdvzZvXVNTUxx66KFx3HHHxVlnnRVjxoxpfa369re/HYsWLYo77rgjDjnkkLxH7baK2JMITelK9ISsaErxFbEpetK1aApZ0JPi0xNqTU/IQlF6Uuolx+t+9rOfxU9+8pPWz/caNWpUfOpTn4pPfepTdZ/lgQceiPe85z11/7pk68knn4ypU6fGww8/HJVKJVJKUalUWo9v3769rvO8+uqr0atXr7p+TbJ14403xumnnx4bNmxoc//AgQPjiiuuiOOPPz6nyfhnRepJhKZ0BXpCLWhKORSpKXrSNWgKWdOTctATsqYnZK0IPekSS46//e1vMXjw4IiIeOaZZ2LevHnR3NwcxxxzTEycOLGus7z+GXa78vpn3lFMU6ZMiR49esS8efNixIgR8Yc//CE2bNgQX/rSl+KSSy6p+3n1ve99r0OP+9znPlfjSXgrXn755fj1r3/d5ofTD3/4w7H77rvnPBmvK1JPIjSlK9ATakVTiq9ITdGTrkFTqAU9KT49IWt6Qi3k3ZNSLzkefvjhmDJlSqxduzYaGxvjpz/9aXzkIx+Jl156KarVarz00ktx/fXXx3HHHVe3marVausW9I3+eTta760onbPXXnvFkiVLYsKECdHQ0BDLli2L0aNHx5IlS+JLX/pS/PGPf6zrPCNGjNjlYyqVSjz55JN1mIbOOvroo2P+/PnR0NAQEREXX3xxzJ49OwYMGBARf/+hdeLEibFq1aocp+zeitiTCE3pCvSErGlK8RWxKXrSNWgKWdKT4tMTakVPyFJRelLqJcdRRx0VPXv2jHPOOSd+9KMfxS233BIf/vCHY968eRERceaZZ8by5cvj3nvvrdtMa9as6dDjhg8fXuNJeCsGDhwYDzzwQIwYMSLe8Y53xLx58+Kwww6LJ554IsaPHx8vv/xy3iNSIj169Ihnn302hgwZEhER/fv3jwcffDBGjhwZERHPPfdc7Lvvvn4QzFERexKhKV2BnpA1TSm+IjZFT7oGTSFLelJ8ekKt6AlZKkpPetb0d6+x++67r3Xz+O53vzvmzp0bc+bMab3o0plnnhnvf//76zrTNddcE2eddZa3dpbcuHHj4qGHHooRI0bEwQcfHN/61rdit912i7lz57Z+k9bThz70oVi4cGHrFpRyeeMuucS75S6riD2J0JSuQE/ImqYUXxGboiddg6aQJT0pPj2hVvSELBWlJ9VcvmpGNmzYEEOHDo2IiD322CP69esXAwcObD0+cODAePHFF+s60/nnnx9btmyp69cke1/72teipaUlIiIuuOCCeOqpp2LixInxy1/+ssOfFZilO++8M7Zt21b3rwvdRRF7EqEpXYGeQPdTxKboSdegKdC96Am1oid0RaV+J0fE3z+T7V/drjd/+6FrOPLII1v/+Z3vfGc8+uijsWHDhhg4cGDu5xjlU6lUCvdaxY6K+BxpSvnpCVnTlHIo2nOkJ12DppAlPSmHoj1HetI16AlZKkpPSr/kmDFjRvTu3TsiIrZu3RqzZ8+Ofv36RUTEK6+8kstMXhC6pkGDBuX69VetWhXr1q37l4+ZMGFCnaahM1JKhXytoq2iPkea0vXoCW+FppRDEZ8jPemaNIU3S0/KoYjPkZ50TXrCm1WUnpT6wuMzZ87s0OOuvvrqGk/yD9VqNRoaGnb5or9hw4Y6TURXUK1Wo1KptPu3Jl6/v1KpuChcQRXxtYq2ivocaQpZ05PyK+rrFf9QxOdIT6gFTSm3Ir5W0VYRnyM9oRb0pNyK8lpV6iVHEVWr1fjud78bDQ0N//Jx06dPr9NEdAXVajWWLVsWe++997983PDhw+s0EVAPmkLW9AS6Jz2hFjQFuh89oRb0hCxYcmSsWq3GunXrYsiQIXmPQhfivILuyfc+WXNOQffke59acF5B9+P7nlpwXpGFat4DdDU+m5C8eDsodD2aQh70BLoePSEvmgJdi56QFz1hVyw5MuaNMdTC5MmTY7fddmv32G9+85s44YQT4m1ve1udpwJqTVPImp5A96Qn1IKmQPejJ9SCnpCFnnkP0NW0tLTkPQJd0B133NHm9po1a+Kqq66Ka665JjZu3BhHHXVUXHvttTlNB9SKppA1PYHuSU+oBU2B7kdPqAU9IQuWHBn7+Mc/3qHHLVy4sMaT0NVs27YtFi5cGPPmzYulS5fG4YcfHn/+85/jj3/8Y4wfPz7v8YAa0BRqQU+g+9ETakVToHvRE2pFT3irLDky1tDQkPcIdEFnnnlmzJ8/PxobG+Pkk0+On/3sZzF48ODo1atX9OjRI+/xgBrRFLKmJ9A96Qm1oCnQ/egJtaAnZKGSfKAeFF7Pnj3jy1/+cpxzzjmx5557tt7fq1eveOihh+Jd73pXjtMBUBZ6AkBWNAWALOgJWXDh8TpZs2ZNrFq1yucX8qb86Ec/imXLlsU+++wTn/zkJ+OWW26J7du35z0WkBNN4c3SE+Cf6QlvhaYAr9MT3go9IQuWHBm76qqr4tJLL21z3+mnnx4jR46M8ePHx7hx42Lt2rU5TUdZnXjiiXH77bfHww8/HGPGjIk5c+bE0KFDo6WlJVatWpX3eECNaApZ0xPonvSEWtAU6H70hFrQE7JgyZGxuXPnxsCBA1tv33bbbXH11VfHtddeG/fdd18MGDAgzj///BwnpMxGjBgR559/fjz99NPx4x//OI4//vg4+eSTY7/99ovPfe5zeY8HZExTqBU9ge5FT6glTYHuQ0+oJT3hrXBNjowNHjw47rzzzhg/fnxERHzmM5+J559/Pq6//vqIiLjzzjtj5syZ8dRTT+U5Jl3Ihg0b4tprr42rr746HnroobzHATKkKdSTnkDXpSfUm6ZA16Qn1Jue0FHeyZGx5ubm6N+/f+vtpqammDRpUuvtkSNHxrp16/IYjS5q0KBB8YUvfMGLPXRBmkI96Ql0XXpCvWkKdE16Qr3pCR3VM+8Buprhw4fH8uXLY/jw4bF+/fpYuXJlHHLIIa3H161bFw0NDTlOSBm98MILMX/+/PjMZz4TEREnnXRSNDc3tx7v2bNnzJ07NwYMGJDThEAtaApZ0xPonvSEWtAU6H70hFrQE7LgnRwZmz59esyZMye+/vWvxyc+8YkYM2ZMHHjgga3Hm5qaYty4cTlOSBn94Ac/iLvvvrv19k033RTVajUaGhqioaEhVqxYEd/97nfzGxCoCU0ha3oC3ZOeUAuaAt2PnlALekIWvJMjY2effXa8/PLLsXDhwhg6dGgsWLCgzfGlS5fGiSeemNN0lNX1118f3/jGN9rc961vfStGjhwZERE33nhjXHDBBXHeeeflMB1QK5pC1vQEuic9oRY0BbofPaEW9IQsuPA4lMDee+8dDzzwQAwbNiwiIg466KD4xS9+Efvtt19ERDz55JMxYcKE2LJlS55jAlBwegJAVjQFgCzoCVnwTo4aaW5ujttvvz0ef/zxiIgYNWpUHHHEEdG3b9+cJ6OMXnrppdi0aVPrC/7999+/w/GWlpY8RgPqQFPIip5A96YnZElToPvSE7KkJ2TBkqMGbrrppjj11FNj/fr1be7fa6+94sorr4wpU6bkNBllNXLkyHjggQd2+tmW999/f4wYMaLOUwH1oClkSU+g+9ITsqYp0D3pCVnTE7LgwuMZa2pqimnTpsWkSZNi6dKlsWHDhtiwYUPcfffdMXHixJg2bVrce++9eY9JyUydOjW+9rWvxXPPPbfDsXXr1sW5554bU6dOzWEyoJY0hazpCXRPekItaAp0P3pCLegJWXBNjowdffTRMWzYsLjiiivaPX7GGWfE2rVr45e//GWdJ6PMXnzxxTj44IPjz3/+c3z605+OUaNGRUTEY489Fj/+8Y/jbW97Wyxbtiz23HPPnCcFsqQpZE1PoHvSE2pBU6D70RNqQU/IgiVHxgYNGhS/+93vYvz48e0eX7FiRUyePDk2btxY58kou40bN8ZXvvKV+PnPfx4vvPBCREQMGDAgTjjhhLjwwgtj0KBB+Q4IZE5TqAU9ge5HT6gVTYHuRU+oFT3hrbLkyFjfvn3j0UcfjeHDh7d7fM2aNTFmzJhobm6u82R0FSmleP755yMiYu+9945KpZLzRECtaAq1pCfQfegJtaYp0D3oCbWmJ7xZrsmRscbGxliyZMlOjy9evDgaGxvrOBFdwV//+tfWf65UKjFkyJAYMmRI64v9a6+9FsuWLctrPKBGNIWs6Ql0T3pCLWgKdD96Qi3oCVmw5MjYzJkz46yzzmr38wdvvfXWOPvss2PGjBn1H4xS22effdq86I8fPz7Wrl3bevtvf/tbfOADH8hjNKCGNIWs6Ql0T3pCLWgKdD96Qi3oCVnomfcAXc3nP//5aGpqio997GMxevToGDt2bKSU4pFHHonVq1fHcccdF1/4whfyHpOSeeOnyj399NPx6quv/svHAOWnKWRNT6B70hNqQVOg+9ETakFPyIJ3cmSsWq3GggUL4qc//WmMHj06Hn300XjsscdizJgxcd1118UNN9wQ1ao/drLncwqh69EU8qAn0PXoCXnRFOha9IS86Am74p0cGdu+fXtccsklcdNNN8W2bdtiypQpcd5550Xfvn3zHg2AktEUALKgJwBkQU+AorJezdiFF14YX/3qV2OPPfaIt73tbfG9730v5syZk/dYlFylUokXX3wxNm/eHJs2bYpKpRJbtmyJzZs3t/4Cuh5NIWt6At2TnlALmgLdj55QC3pCFirJh5plqrGxMc4666w444wzIiLit7/9bXz0ox+N5uZmb9njTatWq23empdSavf29u3b8xgPqBFNIWt6At2TnlALmgLdj55QC3pCFiw5Mta7d+/405/+FMOGDWu9r0+fPvGnP/0p9ttvvxwno8x+97vfdehxkydPrvEkQD1pClnTE+ie9IRa0BTofvSEWtATsuCaHBl77bXXok+fPm3u69WrV7z66qs5TURX4IUcuidNIWt6At2TnlALmgLdj55QC3pCFiw5MpZSihkzZkTv3r1b79u6dWvMnj07+vXr13rfwoUL8xiPknrjW/faU6lU4rXXXqvTREA9aApZ0xPonvSEWtAU6H70hFrQE7JgyZGx6dOn73DfySefnMMkdCU33njjTo/dc8898b3vfS9aWlrqOBFQD5pC1vQEuic9oRY0BbofPaEW9IQsuCYHlNRjjz0W55xzTtx8881x0kknxQUXXBDDhw/PeywASkZPAMiKpgCQBT2hs6p5DwB0zl/+8pc47bTTYvz48fHaa6/Fgw8+GNdcc40XewA6RU8AyIqmAJAFPeHNsuSAkti0aVN8+ctfjne+852xcuXKWLx4cdx8880xbty4vEcDoET0BICsaAoAWdAT3irX5IAS+Na3vhXf/OY3Y+jQoTF//vw49thj8x4JgBLSEwCyoikAZEFPyIJrckAJVKvV6Nu3bxx++OHRo0ePnT5u4cKFdZwKgLLREwCyoikAZEFPyIJ3ckAJnHLKKVGpVPIeA4CS0xMAsqIpAGRBT8iCd3IAAAAAAACl5MLjAAAAAABAKVlyAAAAAAAApWTJAQAAAAAAlJIlBwAAAAAAUEqWHAAAAAAAQClZcgAAAAAAAKVkyQEAAAAAAJSSJQcAAAAAAFBK/x8g8S07JbqRTQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Block number  4\n",
            "Decoder Self Attention 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Cross attention 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for layer in range(num_dec_layers):\n",
        "    fig, axs = plt.subplots(1, num_heads, figsize=(20, 10))\n",
        "\n",
        "    print(\"Decoder Block number \", layer + 1)\n",
        "\n",
        "    print(\"Decoder Self Attention\", layer + 1)\n",
        "    for h in range(num_heads):\n",
        "        draw(\n",
        "            trained_model.decoder.layers[layer]\n",
        "            .attention_self.heads[h]\n",
        "            .weights_softmax.data.cpu()\n",
        "            .numpy()[0],\n",
        "            target_exp,\n",
        "            target_exp if h == 0 else [],\n",
        "            ax=axs[h],\n",
        "        )\n",
        "    plt.show()\n",
        "    print(\"Decoder Cross attention\", layer + 1)\n",
        "    fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
        "    for h in range(num_heads):\n",
        "        draw(\n",
        "            trained_model.decoder.layers[layer]\n",
        "            .attention_cross.heads[h]\n",
        "            .weights_softmax.data.cpu()\n",
        "            .numpy()[0],\n",
        "            inp_seq,\n",
        "            target_exp if h == 0 else [],\n",
        "            ax=axs[h],\n",
        "        )\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8a52caf-c12e-4ca9-986b-cc52fe50545a",
      "metadata": {
        "id": "c8a52caf-c12e-4ca9-986b-cc52fe50545a"
      },
      "source": [
        "# Submit Your Work\n",
        "After completing both notebooks for this assignment (`transformers.ipynb` and this notebook, `rnn_lstm_captionaing.ipynb`), run the following cell to create a `.zip` file for you to download and turn in.\n",
        "\n",
        "**Please MANUALLY SAVE every `*.ipynb` and `*.py` files before executing the following cell:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82833983",
      "metadata": {
        "id": "82833983"
      },
      "outputs": [],
      "source": [
        "from eecs598.submit import make_a5_submission\n",
        "\n",
        "# TODO: Replace these with your actual uniquename and umid\n",
        "uniquename = None\n",
        "umid = None\n",
        "make_a5_submission(GOOGLE_DRIVE_PATH, uniquename, umid)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "b43b5e66-7d96-49a7-8d73-649c1d8de2ef",
        "137296b8-8ab8-4f9d-bff5-e2584370a757",
        "3412c073-d239-450a-aa46-9ec3d61309a6",
        "0b9ae8cd-2813-4845-a4df-d47ebdc60971"
      ],
      "provenance": []
    },
    "interpreter": {
      "hash": "3e6a8e772529b48ea93620fbc55d49ea9e469a86dedbc53ac24607b5264d00e5"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}